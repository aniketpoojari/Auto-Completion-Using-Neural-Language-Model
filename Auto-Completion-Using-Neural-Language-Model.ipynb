{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2842,
     "status": "ok",
     "timestamp": 1614659569129,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "qogGakq2-uqD",
    "outputId": "d211fa9c-c3a4-4f80-c460-a500cc7fc895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZoPOnsX8uPS"
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5284,
     "status": "ok",
     "timestamp": 1614659578587,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "Z_CCxOEI4iZK",
    "outputId": "be8e4092-0ab1-4e55-ed96-6020fdc60d78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f064834caf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 2019\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_5gPvXxWjru"
   },
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3856,
     "status": "ok",
     "timestamp": 1614659586887,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "DJGwtHYz-67g"
   },
   "outputs": [],
   "source": [
    "dir = \"drive/MyDrive/Auto-Completion-Using-Neural-Language-Model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5711,
     "status": "ok",
     "timestamp": 1614659593734,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "RTcAv8MwCUep"
   },
   "outputs": [],
   "source": [
    "with open(dir + \"Dailog-Dataset.dialogs_dataset\", \"rb\") as f:\n",
    "  dialogs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3101,
     "status": "ok",
     "timestamp": 1614659598329,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "81_SXWZlE6Zb",
    "outputId": "0a1070e3-7ad3-41a7-eef8-a57071fcd863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64776"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1614659693170,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "zSyAzbttAqP2",
    "outputId": "96b4682c-8d1d-4545-9488-0b380577d2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's do that one then\",\n",
       " 'I can do that',\n",
       " \"That's what I'm talking about\",\n",
       " 'Thanks so much Donna!',\n",
       " 'Well, my car is making a strange noise, so I need to take it to the car repair shop']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Thanks again, goodbye',\n",
       " 'What about a table',\n",
       " 'I want Italian sausage and mushrooms',\n",
       " 'Yes can you order me a pizza from Pizza Hut?',\n",
       " 'Can I also add a medium pizza just plain cheese']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(dialogs, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGGTGRUDW9I3"
   },
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crvUl_ngM8sb"
   },
   "source": [
    "## TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1614659934515,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "kUuUKZgUFhkl"
   },
   "outputs": [],
   "source": [
    "dialogs_clean = []\n",
    "\n",
    "for i in dialogs:\n",
    "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "  i = i.lower()\n",
    "  dialogs_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1978,
     "status": "ok",
     "timestamp": 1614659938284,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "DgZwFK1eSjkN",
    "outputId": "4642b856-6b2e-4fa7-8cdc-b6781cd111b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a mocha frappuccino with whipped cream',\n",
       " ' that is all',\n",
       " 'hey i need you to schedule me an appointment at a repair shop',\n",
       " \"hi i'd like call in a lyft\",\n",
       " \"i'd like to see the movie alita battle angel\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(dialogs_clean, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYDzFQVvNA7_"
   },
   "source": [
    "\n",
    "## WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2293,
     "status": "ok",
     "timestamp": 1614659948666,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "V1hrCYRp11UI"
   },
   "outputs": [],
   "source": [
    "all_words = \" \".join(dialogs_clean).split()\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "for word in all_words:   \n",
    "  if word in words_dict:\n",
    "    words_dict[word] = words_dict[word] + 1\n",
    "  else:\n",
    "    words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3212,
     "status": "ok",
     "timestamp": 1614659952410,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "gNxSGPubWaqA"
   },
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
    "words_df = words_df.sort_values(by = ['count'])\n",
    "words_df.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2483,
     "status": "ok",
     "timestamp": 1614659958176,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "KVPbPsSWo-Ak",
    "outputId": "672565f1-5ae1-48a4-c3cb-f06feef34c9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 3255,
     "status": "ok",
     "timestamp": 1614659962184,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "JTwmmOiEXBHt",
    "outputId": "6d7410d9-63b9-483d-a127-824dc9e4d2ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppermiddle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shoots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andrea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0  uppermiddle      1\n",
       "1       shoots      1\n",
       "2        geesh      1\n",
       "3       andrea      1\n",
       "4      precice      1"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 2224,
     "status": "ok",
     "timestamp": 1614659966593,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "EWJEto8TMPiq",
    "outputId": "1c72c6ff-827e-4555-d91f-184b53cc7930"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11142</th>\n",
       "      <td>you</td>\n",
       "      <td>11909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11143</th>\n",
       "      <td>a</td>\n",
       "      <td>13380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>to</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>the</td>\n",
       "      <td>15406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>i</td>\n",
       "      <td>19654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "11142  you  11909\n",
       "11143    a  13380\n",
       "11144   to  14000\n",
       "11145  the  15406\n",
       "11146    i  19654"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_XbqD-fPAwL"
   },
   "source": [
    "## REPLACE RARE WORDS WITH <unk> TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1479,
     "status": "ok",
     "timestamp": 1614660041745,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "TZU1YctPBO5t"
   },
   "outputs": [],
   "source": [
    "rare_thresh = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1614660045225,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "1iC4ztG3XIP3",
    "outputId": "c91e6ce3-1a64-4aec-8a26-823643bde2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words distribution in the vocabulary: 69.03\n"
     ]
    }
   ],
   "source": [
    "# get percentage of rare words in the vocabulary\n",
    "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
    "total_words = len(words_df) \n",
    "rare_dist = rare_words_count / total_words\n",
    "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1614660046851,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "helYHQ4BXNK9",
    "outputId": "5456e4ed-96ee-4c3a-d75f-e888508d4933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words coverage in the corpus: 2.27\n"
     ]
    }
   ],
   "source": [
    "# coverage percentage of rare words in the corpus\n",
    "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()\n",
    "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1614660050331,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "yJhbRQllXQJk"
   },
   "outputs": [],
   "source": [
    "# extract rare words in a list\n",
    "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1614660056874,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "0f4QyoOmCWrt"
   },
   "outputs": [],
   "source": [
    "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
    "pattern = \"\"\n",
    "for i in rare_words:\n",
    "  pattern += \" \" + i + \" |\"\n",
    "pattern = pattern[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "ae48cfe6dc1b44829c03769bf7b6583b",
      "f940c82a1d2a4317bd18be329d9280c6",
      "7511ca3a1b4249c18b725a02594ea5e6",
      "0619c4ac931941a5a2aff2e4a352f793",
      "a240d19768074d049cae40af9ac869e4",
      "40f2c3a49a4c4026a0b107b8c2b6ec6c",
      "32d382d3351e4d2da606c0eb54a98660",
      "79b97eb00f1c4b0fb24a63f8a9558f4b"
     ]
    },
    "executionInfo": {
     "elapsed": 21244,
     "status": "ok",
     "timestamp": 1614660078686,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "ABXGV4_-CWjr",
    "outputId": "567e5da8-3d74-46fa-a42f-7c9cb8e2b2b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae48cfe6dc1b44829c03769bf7b6583b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=64776.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dialogs_clean_v2 = []\n",
    "for d in tqdm_notebook(dialogs_clean):\n",
    "  text = re.sub(pattern, \" <unk> \", d)\n",
    "  dialogs_clean_v2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3175,
     "status": "ok",
     "timestamp": 1614660571937,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "SoQwO5FAZSP1",
    "outputId": "2dd40dcc-48e4-454d-f4ca-3b7d2a6699d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay i will pay you back when you get here',\n",
       " 'thanks have a good night',\n",
       " 'yes could you direct me to the restroom',\n",
       " 'can you find me seafood restaurants in the area',\n",
       " 'ok sounds good to me',\n",
       " 'im thinking about an iced <unk> latte',\n",
       " 'no thank you',\n",
       " 'hi i want movie tickets',\n",
       " 'a peppermint mocha frappachino',\n",
       " 'yes that one']"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(dialogs_clean_v2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waTukoy1AbVT"
   },
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SvIJQ0brBWW"
   },
   "source": [
    "## PREPARE SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 3152,
     "status": "ok",
     "timestamp": 1614660594298,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "H5BYGcwBF7hX",
    "outputId": "43322754-9464-44dc-99bc-f4d0867db325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f064368bb50>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the sequences\n",
    "text_word_count = []\n",
    "for i in dialogs_clean_v2:\n",
    "  text_word_count.append(len(i.split()))\n",
    "        \n",
    "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2083,
     "status": "ok",
     "timestamp": 1614660605988,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "c-b38u20E5Pz"
   },
   "outputs": [],
   "source": [
    "def create_seq(text, seq_len = 5):\n",
    "      \n",
    "  sequences = []    \n",
    "  \n",
    "  if len(text.split()) > seq_len:\n",
    "    for i in range(0, len(text.split()) - seq_len):\n",
    "      seq = text.split()[i:i + seq_len + 1]\n",
    "      sequences.append(\" \".join(seq))\n",
    "    return sequences\n",
    "\n",
    "  else:\n",
    "    \n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3290,
     "status": "ok",
     "timestamp": 1614660609124,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "p0psJCgo9QtH"
   },
   "outputs": [],
   "source": [
    "# create sequences of equal length\n",
    "seqs = [create_seq(i) for i in dialogs_clean_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1622,
     "status": "ok",
     "timestamp": 1614660611884,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "LCrf5t5vCI6I",
    "outputId": "8f625b7f-a423-4e0d-ef0f-a05013281314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"hi i'm looking to book a\",\n",
       "  \"i'm looking to book a table\",\n",
       "  'looking to book a table for',\n",
       "  'to book a table for korean',\n",
       "  'book a table for korean fod'],\n",
       " ['somewhere in southern nyc maybe the',\n",
       "  'in southern nyc maybe the east',\n",
       "  'southern nyc maybe the east village']]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 56812,
     "status": "ok",
     "timestamp": 1614660672114,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "S4O3-S6PA78w"
   },
   "outputs": [],
   "source": [
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54221,
     "status": "ok",
     "timestamp": 1614660672115,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "dwmBsxuqxPRq",
    "outputId": "d2a57b1f-6294-41d0-d5b1-3a48607b0ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book a\",\n",
       " \"i'm looking to book a table\",\n",
       " 'looking to book a table for',\n",
       " 'to book a table for korean',\n",
       " 'book a table for korean fod',\n",
       " 'somewhere in southern nyc maybe the',\n",
       " 'in southern nyc maybe the east',\n",
       " 'southern nyc maybe the east village',\n",
       " \"we don't want to sit at\",\n",
       " \"don't want to sit at the\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1614662580938,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "cNW5MIpTDufa",
    "outputId": "1821b30b-5ff1-461f-f0c2-0f7eb0f792b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205346"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1160,
     "status": "ok",
     "timestamp": 1614662587384,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "M0dppnffaEgG"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1614662589176,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "27XKPYgLwuOF",
    "outputId": "1c63c5ef-e204-4eaa-d002-a738024b0029"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hi i'm looking to book\", \"i'm looking to book a\")"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1614662592108,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "rVge-c6WHUx1",
    "outputId": "52d18e0f-204e-42d6-c655-736bf0795fa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to drive to several locations', 'drive to several locations do')"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[88543], y[88543]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzR5aG9wrZJ5"
   },
   "source": [
    "## CREATE TOKEN-INTEGER MAPPING AND VICE VERSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1614662598596,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "CSgaezAI60ht"
   },
   "outputs": [],
   "source": [
    "int2token = {}\n",
    "cnt = 1\n",
    "\n",
    "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "token2int = {t: i for i, t in int2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1614662600955,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "j5TSElOk_OQf",
    "outputId": "9506f45f-71c2-401b-f911-c6a489a9cfad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1726, 'rotor')"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2int[\"can\"], int2token[1127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQOk80iBr51V"
   },
   "source": [
    "## TRAIN / VAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1614662611258,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "Jn6a1aXvAcRl"
   },
   "outputs": [],
   "source": [
    "x_tr = x[:150000]\n",
    "y_tr = y[:150000]\n",
    "\n",
    "x_val = x[150000:]\n",
    "y_val = y[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIfKsjE3sxgu"
   },
   "source": [
    "## PAD SEQUENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1614662618996,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "QlG-sd79sNjH",
    "outputId": "9275b59f-537b-494e-86e1-d4155ffb089d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0640172ed0>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sequence length in train set\n",
    "text_word_count = []\n",
    "\n",
    "for i in x_tr:\n",
    "  text_word_count.append(len(i.split()))\n",
    "\n",
    "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1614662622785,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "2umo7Mns5NKl"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(seq, n):\n",
    "\n",
    "  seq = seq.split()\n",
    "  \n",
    "  if len(seq) < n:\n",
    "    for i in range(n - len(seq)):\n",
    "      seq.append(\"<pad>\")\n",
    "\n",
    "  return \" \".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1614662624741,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "jKXmzvuoKh79"
   },
   "outputs": [],
   "source": [
    "# based on the plot above\n",
    "max_text_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1291,
     "status": "ok",
     "timestamp": 1614662626533,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "g7Fl389QKdRE"
   },
   "outputs": [],
   "source": [
    "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
    "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
    "\n",
    "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
    "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1614662630252,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "gxwn3NyD-HBD",
    "outputId": "a70ef091-a6de-43e3-826d-49e0e6873c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is a <unk> for the',\n",
       " 'try and get me a',\n",
       " 'you also get me a',\n",
       " 'and the date is fast',\n",
       " 'what times are available are',\n",
       " 'ok that one will <pad>',\n",
       " 'what kind of snacks do',\n",
       " 'so now <pad> <pad> <pad>',\n",
       " 'really want the pizza and',\n",
       " 'and how much is the']"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(x_tr_padded, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1614662637460,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "mlUnd9v_-4ad"
   },
   "outputs": [],
   "source": [
    "# update mapping dictionaries\n",
    "int2token[0] = \"<pad>\"\n",
    "token2int[\"<pad>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1614662645273,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "oXNDs-bLLtoG"
   },
   "outputs": [],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLqEIRavtMpY"
   },
   "source": [
    "## CONVERT TEXT TO INTEGER SEQUENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1614662649796,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "HVxwznePACXC"
   },
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 2029,
     "status": "ok",
     "timestamp": 1614662652263,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "r7QVnobNARnr"
   },
   "outputs": [],
   "source": [
    "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
    "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
    "\n",
    "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
    "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1614662654603,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "qo8g1L_FwAOC",
    "outputId": "28dc8938-bd8d-436b-a2c1-f38bcd99951a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1774, 6144, 6211, 6021, 4981],\n",
       " [6144, 6211, 6021, 4981, 6239],\n",
       " [6211, 6021, 4981, 6239, 5645],\n",
       " [6021, 4981, 6239, 5645, 2897],\n",
       " [4981, 6239, 5645, 2897, 5133],\n",
       " [2734, 1935, 214, 3116, 4554],\n",
       " [1935, 214, 3116, 4554, 238],\n",
       " [214, 3116, 4554, 238, 2919],\n",
       " [684, 3507, 128, 6021, 691],\n",
       " [3507, 128, 6021, 691, 3757]]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1614662655829,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "yGa_ORxZwDkw",
    "outputId": "f07fb8b3-f2c6-4a4e-e4fc-2ccc326d19e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6144, 6211, 6021, 4981, 6239],\n",
       " [6211, 6021, 4981, 6239, 5645],\n",
       " [6021, 4981, 6239, 5645, 2897],\n",
       " [4981, 6239, 5645, 2897, 5133],\n",
       " [6239, 5645, 2897, 5133, 4389],\n",
       " [1935, 214, 3116, 4554, 238],\n",
       " [214, 3116, 4554, 238, 2919],\n",
       " [3116, 4554, 238, 2919, 4840],\n",
       " [3507, 128, 6021, 691, 3757],\n",
       " [128, 6021, 691, 3757, 238]]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1614662661093,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "eLntLMs5NwGi",
    "outputId": "b9f264ca-a004-4f4f-8d3f-7b4ea29f3821"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_int = np.array(x_tr_int)\n",
    "y_tr_int = np.array(y_tr_int)\n",
    "\n",
    "x_val_int = np.array(x_val_int)\n",
    "y_val_int = np.array(y_val_int)\n",
    "\n",
    "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y3bZcFywH33"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCh2prnryopJ"
   },
   "source": [
    "## ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1614662938442,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "ldcidFnPRsOM"
   },
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "      \n",
    "  def __init__(self, n_hidden = 256, n_layers = 2, drop_prob = 0.3, lr = 0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_layers = n_layers\n",
    "    self.n_hidden = n_hidden\n",
    "    self.lr = lr\n",
    "    \n",
    "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
    "    \n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "  \n",
    "  def forward(self, x, hidden):\n",
    "    \n",
    "    embedded = self.emb_layer(x)     \n",
    "    \n",
    "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "    \n",
    "    out = self.dropout(lstm_output)\n",
    "    \n",
    "    out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "    out = self.fc(out)\n",
    "\n",
    "    return out, hidden\n",
    "    \n",
    "    \n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1614662941887,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "LQq72w9ADLn_",
    "outputId": "949c8c8e-8a4f-457e-bffe-5841b519090a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(6502, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WordLSTM()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1614662950730,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "2pRtYvAVFCKB"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "  prv = 0\n",
    "  \n",
    "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "    x = arr_x[prv:n,:]\n",
    "    y = arr_y[prv:n,:]\n",
    "    prv = n\n",
    "    yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpV3vbVeytW1"
   },
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1614663174179,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "QHMBy8HQ2GhY"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
    "      \n",
    "  best_valid_loss = float('inf')\n",
    "  \n",
    "  opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "  \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "    net.cuda()\n",
    "  \n",
    "  counter = 0\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for e in range(epochs):\n",
    "            \n",
    "\n",
    "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
    "      counter+= 1\n",
    "      \n",
    "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "      \n",
    "      if(torch.cuda.is_available()):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "      h = net.init_hidden(batch_size)\n",
    "\n",
    "      # set accumulated gradients to zero\n",
    "      net.zero_grad()\n",
    "      \n",
    "      output, h = net(inputs, h)\n",
    "      \n",
    "      loss = criterion(output, targets.view(-1))\n",
    "      \n",
    "      loss.backward()\n",
    "      \n",
    "      opt.step()\n",
    "      \n",
    "      if counter % print_every == 0:\n",
    "        \n",
    "        val_losses = []\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
    "            \n",
    "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "          \n",
    "          val_h = net.init_hidden(batch_size)\n",
    "\n",
    "          inputs, targets = x, y\n",
    "          \n",
    "          if(torch.cuda.is_available()):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "          output, val_h = net(inputs, val_h)\n",
    "\n",
    "          val_loss = criterion(output, targets.view(-1))\n",
    "          val_losses.append(val_loss.item())\n",
    "\n",
    "        #save the best model\n",
    "        if np.mean(val_losses) < best_valid_loss:\n",
    "          best_valid_loss = np.mean(val_losses)\n",
    "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "      \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.4f}...\".format(loss.item()),\n",
    "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 852317,
     "status": "ok",
     "timestamp": 1614664036505,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "sbGexKELaTNb",
    "outputId": "8f4207cf-8ca8-4bf7-ad09-4984c672a93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 32... Loss: 6.4242... ppl: 675.5272  Val Loss: 6.5155\n",
      "Epoch: 1/10... Step: 64... Loss: 5.6527... ppl: 340.7983  Val Loss: 5.8313\n",
      "Epoch: 1/10... Step: 96... Loss: 6.0215... ppl: 270.7575  Val Loss: 5.6012\n",
      "Epoch: 1/10... Step: 128... Loss: 5.2435... ppl: 233.4948  Val Loss: 5.4532\n",
      "Epoch: 1/10... Step: 160... Loss: 6.1583... ppl: 205.0417  Val Loss: 5.3232\n",
      "Epoch: 1/10... Step: 192... Loss: 5.7276... ppl: 181.2442  Val Loss: 5.1998\n",
      "Epoch: 1/10... Step: 224... Loss: 5.1447... ppl: 163.8765  Val Loss: 5.0991\n",
      "Epoch: 1/10... Step: 256... Loss: 4.2683... ppl: 149.1482  Val Loss: 5.0049\n",
      "Epoch: 1/10... Step: 288... Loss: 4.8150... ppl: 136.9335  Val Loss: 4.9195\n",
      "Epoch: 1/10... Step: 320... Loss: 4.9435... ppl: 127.6104  Val Loss: 4.8490\n",
      "Epoch: 1/10... Step: 352... Loss: 4.3801... ppl: 119.3279  Val Loss: 4.7819\n",
      "Epoch: 1/10... Step: 384... Loss: 5.0353... ppl: 112.3799  Val Loss: 4.7219\n",
      "Epoch: 1/10... Step: 416... Loss: 4.9005... ppl: 106.7800  Val Loss: 4.6708\n",
      "Epoch: 1/10... Step: 448... Loss: 4.5941... ppl: 102.3586  Val Loss: 4.6285\n",
      "Epoch: 1/10... Step: 480... Loss: 4.3569... ppl: 98.5283  Val Loss: 4.5903\n",
      "Epoch: 1/10... Step: 512... Loss: 4.4829... ppl: 94.6531  Val Loss: 4.5502\n",
      "Epoch: 1/10... Step: 544... Loss: 4.7046... ppl: 91.4221  Val Loss: 4.5155\n",
      "Epoch: 1/10... Step: 576... Loss: 5.4897... ppl: 88.4346  Val Loss: 4.4823\n",
      "Epoch: 1/10... Step: 608... Loss: 4.4647... ppl: 86.2835  Val Loss: 4.4576\n",
      "Epoch: 1/10... Step: 640... Loss: 4.3102... ppl: 83.4300  Val Loss: 4.4240\n",
      "Epoch: 1/10... Step: 672... Loss: 4.3728... ppl: 82.0756  Val Loss: 4.4076\n",
      "Epoch: 1/10... Step: 704... Loss: 4.2523... ppl: 79.3739  Val Loss: 4.3742\n",
      "Epoch: 1/10... Step: 736... Loss: 4.8201... ppl: 78.3901  Val Loss: 4.3617\n",
      "Epoch: 1/10... Step: 768... Loss: 3.8769... ppl: 75.7082  Val Loss: 4.3269\n",
      "Epoch: 1/10... Step: 800... Loss: 4.2051... ppl: 74.3162  Val Loss: 4.3083\n",
      "Epoch: 1/10... Step: 832... Loss: 4.8158... ppl: 72.8074  Val Loss: 4.2878\n",
      "Epoch: 1/10... Step: 864... Loss: 4.0943... ppl: 71.7237  Val Loss: 4.2728\n",
      "Epoch: 1/10... Step: 896... Loss: 4.2426... ppl: 70.2533  Val Loss: 4.2521\n",
      "Epoch: 1/10... Step: 928... Loss: 4.3522... ppl: 68.3775  Val Loss: 4.2250\n",
      "Epoch: 1/10... Step: 960... Loss: 4.0086... ppl: 67.9297  Val Loss: 4.2185\n",
      "Epoch: 1/10... Step: 992... Loss: 3.7572... ppl: 66.1818  Val Loss: 4.1924\n",
      "Epoch: 1/10... Step: 1024... Loss: 3.6879... ppl: 65.0725  Val Loss: 4.1755\n",
      "Epoch: 1/10... Step: 1056... Loss: 4.2213... ppl: 64.0075  Val Loss: 4.1590\n",
      "Epoch: 1/10... Step: 1088... Loss: 3.4807... ppl: 62.9678  Val Loss: 4.1426\n",
      "Epoch: 1/10... Step: 1120... Loss: 4.0821... ppl: 62.1161  Val Loss: 4.1290\n",
      "Epoch: 1/10... Step: 1152... Loss: 3.2336... ppl: 61.2275  Val Loss: 4.1146\n",
      "Epoch: 1/10... Step: 1184... Loss: 3.9921... ppl: 60.4830  Val Loss: 4.1024\n",
      "Epoch: 1/10... Step: 1216... Loss: 3.7463... ppl: 59.6826  Val Loss: 4.0890\n",
      "Epoch: 1/10... Step: 1248... Loss: 4.1563... ppl: 58.6065  Val Loss: 4.0708\n",
      "Epoch: 1/10... Step: 1280... Loss: 4.2459... ppl: 57.8900  Val Loss: 4.0585\n",
      "Epoch: 1/10... Step: 1312... Loss: 4.9123... ppl: 57.7660  Val Loss: 4.0564\n",
      "Epoch: 1/10... Step: 1344... Loss: 3.8363... ppl: 56.9555  Val Loss: 4.0423\n",
      "Epoch: 1/10... Step: 1376... Loss: 4.1868... ppl: 55.9151  Val Loss: 4.0238\n",
      "Epoch: 1/10... Step: 1408... Loss: 3.5887... ppl: 55.2988  Val Loss: 4.0128\n",
      "Epoch: 1/10... Step: 1440... Loss: 4.0472... ppl: 54.7342  Val Loss: 4.0025\n",
      "Epoch: 1/10... Step: 1472... Loss: 3.4492... ppl: 54.3588  Val Loss: 3.9956\n",
      "Epoch: 1/10... Step: 1504... Loss: 4.1544... ppl: 53.9797  Val Loss: 3.9886\n",
      "Epoch: 1/10... Step: 1536... Loss: 4.8221... ppl: 53.6541  Val Loss: 3.9826\n",
      "Epoch: 1/10... Step: 1568... Loss: 3.9119... ppl: 52.5492  Val Loss: 3.9617\n",
      "Epoch: 1/10... Step: 1600... Loss: 3.7491... ppl: 52.0896  Val Loss: 3.9530\n",
      "Epoch: 1/10... Step: 1632... Loss: 5.2365... ppl: 51.4520  Val Loss: 3.9406\n",
      "Epoch: 1/10... Step: 1664... Loss: 4.0342... ppl: 50.9543  Val Loss: 3.9309\n",
      "Epoch: 1/10... Step: 1696... Loss: 3.6217... ppl: 50.5747  Val Loss: 3.9235\n",
      "Epoch: 1/10... Step: 1728... Loss: 3.5644... ppl: 50.0415  Val Loss: 3.9129\n",
      "Epoch: 1/10... Step: 1760... Loss: 4.4414... ppl: 49.8898  Val Loss: 3.9098\n",
      "Epoch: 1/10... Step: 1792... Loss: 3.4207... ppl: 49.4872  Val Loss: 3.9017\n",
      "Epoch: 1/10... Step: 1824... Loss: 4.3507... ppl: 49.8199  Val Loss: 3.9084\n",
      "Epoch: 1/10... Step: 1856... Loss: 5.7048... ppl: 48.7945  Val Loss: 3.8876\n",
      "Epoch: 1/10... Step: 1888... Loss: 3.6224... ppl: 48.1066  Val Loss: 3.8734\n",
      "Epoch: 1/10... Step: 1920... Loss: 3.5441... ppl: 47.7676  Val Loss: 3.8663\n",
      "Epoch: 1/10... Step: 1952... Loss: 2.9694... ppl: 47.4481  Val Loss: 3.8596\n",
      "Epoch: 1/10... Step: 1984... Loss: 2.8898... ppl: 47.4797  Val Loss: 3.8603\n",
      "Epoch: 1/10... Step: 2016... Loss: 3.5881... ppl: 46.8071  Val Loss: 3.8460\n",
      "Epoch: 1/10... Step: 2048... Loss: 4.0876... ppl: 46.3899  Val Loss: 3.8371\n",
      "Epoch: 1/10... Step: 2080... Loss: 3.6994... ppl: 46.2179  Val Loss: 3.8334\n",
      "Epoch: 1/10... Step: 2112... Loss: 4.7741... ppl: 46.1663  Val Loss: 3.8323\n",
      "Epoch: 1/10... Step: 2144... Loss: 3.4712... ppl: 45.6983  Val Loss: 3.8221\n",
      "Epoch: 1/10... Step: 2176... Loss: 5.1909... ppl: 45.4419  Val Loss: 3.8164\n",
      "Epoch: 1/10... Step: 2208... Loss: 4.1858... ppl: 45.2983  Val Loss: 3.8133\n",
      "Epoch: 1/10... Step: 2240... Loss: 3.6754... ppl: 45.2583  Val Loss: 3.8124\n",
      "Epoch: 1/10... Step: 2272... Loss: 3.9009... ppl: 44.5364  Val Loss: 3.7963\n",
      "Epoch: 1/10... Step: 2304... Loss: 3.2997... ppl: 44.2133  Val Loss: 3.7890\n",
      "Epoch: 1/10... Step: 2336... Loss: 4.3136... ppl: 44.0635  Val Loss: 3.7856\n",
      "Epoch: 2/10... Step: 2368... Loss: 3.2904... ppl: 44.1892  Val Loss: 3.7885\n",
      "Epoch: 2/10... Step: 2400... Loss: 3.4282... ppl: 43.9609  Val Loss: 3.7833\n",
      "Epoch: 2/10... Step: 2432... Loss: 3.8052... ppl: 44.1371  Val Loss: 3.7873\n",
      "Epoch: 2/10... Step: 2464... Loss: 4.2440... ppl: 43.8602  Val Loss: 3.7810\n",
      "Epoch: 2/10... Step: 2496... Loss: 4.0020... ppl: 43.4421  Val Loss: 3.7714\n",
      "Epoch: 2/10... Step: 2528... Loss: 3.4643... ppl: 43.2663  Val Loss: 3.7674\n",
      "Epoch: 2/10... Step: 2560... Loss: 5.2030... ppl: 42.9881  Val Loss: 3.7609\n",
      "Epoch: 2/10... Step: 2592... Loss: 4.1315... ppl: 43.1742  Val Loss: 3.7652\n",
      "Epoch: 2/10... Step: 2624... Loss: 3.2303... ppl: 42.8597  Val Loss: 3.7579\n",
      "Epoch: 2/10... Step: 2656... Loss: 3.8534... ppl: 42.7193  Val Loss: 3.7547\n",
      "Epoch: 2/10... Step: 2688... Loss: 3.3669... ppl: 42.3564  Val Loss: 3.7461\n",
      "Epoch: 2/10... Step: 2720... Loss: 3.7556... ppl: 41.9983  Val Loss: 3.7376\n",
      "Epoch: 2/10... Step: 2752... Loss: 3.6126... ppl: 42.1955  Val Loss: 3.7423\n",
      "Epoch: 2/10... Step: 2784... Loss: 4.1651... ppl: 42.0301  Val Loss: 3.7384\n",
      "Epoch: 2/10... Step: 2816... Loss: 4.0765... ppl: 41.9548  Val Loss: 3.7366\n",
      "Epoch: 2/10... Step: 2848... Loss: 3.5375... ppl: 41.7064  Val Loss: 3.7307\n",
      "Epoch: 2/10... Step: 2880... Loss: 4.1410... ppl: 41.6431  Val Loss: 3.7291\n",
      "Epoch: 2/10... Step: 2912... Loss: 3.2116... ppl: 41.3112  Val Loss: 3.7211\n",
      "Epoch: 2/10... Step: 2944... Loss: 3.3406... ppl: 41.4137  Val Loss: 3.7236\n",
      "Epoch: 2/10... Step: 2976... Loss: 3.3509... ppl: 41.0470  Val Loss: 3.7147\n",
      "Epoch: 2/10... Step: 3008... Loss: 4.1220... ppl: 41.0079  Val Loss: 3.7138\n",
      "Epoch: 2/10... Step: 3040... Loss: 3.5389... ppl: 40.8247  Val Loss: 3.7093\n",
      "Epoch: 2/10... Step: 3072... Loss: 3.6209... ppl: 40.6627  Val Loss: 3.7053\n",
      "Epoch: 2/10... Step: 3104... Loss: 3.7356... ppl: 40.5208  Val Loss: 3.7018\n",
      "Epoch: 2/10... Step: 3136... Loss: 4.0629... ppl: 40.3848  Val Loss: 3.6985\n",
      "Epoch: 2/10... Step: 3168... Loss: 3.4457... ppl: 40.2353  Val Loss: 3.6947\n",
      "Epoch: 2/10... Step: 3200... Loss: 3.8386... ppl: 40.3775  Val Loss: 3.6983\n",
      "Epoch: 2/10... Step: 3232... Loss: 4.2215... ppl: 39.9895  Val Loss: 3.6886\n",
      "Epoch: 2/10... Step: 3264... Loss: 2.8491... ppl: 39.7127  Val Loss: 3.6817\n",
      "Epoch: 2/10... Step: 3296... Loss: 4.1958... ppl: 39.8104  Val Loss: 3.6841\n",
      "Epoch: 2/10... Step: 3328... Loss: 4.1265... ppl: 39.5856  Val Loss: 3.6785\n",
      "Epoch: 2/10... Step: 3360... Loss: 3.5976... ppl: 39.2953  Val Loss: 3.6711\n",
      "Epoch: 2/10... Step: 3392... Loss: 3.2362... ppl: 39.7442  Val Loss: 3.6825\n",
      "Epoch: 2/10... Step: 3424... Loss: 4.3123... ppl: 39.2385  Val Loss: 3.6697\n",
      "Epoch: 2/10... Step: 3456... Loss: 3.4287... ppl: 39.0811  Val Loss: 3.6656\n",
      "Epoch: 2/10... Step: 3488... Loss: 3.6125... ppl: 39.2915  Val Loss: 3.6710\n",
      "Epoch: 2/10... Step: 3520... Loss: 3.4340... ppl: 38.9339  Val Loss: 3.6619\n",
      "Epoch: 2/10... Step: 3552... Loss: 4.0288... ppl: 38.9561  Val Loss: 3.6624\n",
      "Epoch: 2/10... Step: 3584... Loss: 2.9929... ppl: 38.6900  Val Loss: 3.6556\n",
      "Epoch: 2/10... Step: 3616... Loss: 3.5850... ppl: 38.5645  Val Loss: 3.6523\n",
      "Epoch: 2/10... Step: 3648... Loss: 3.1748... ppl: 38.6978  Val Loss: 3.6558\n",
      "Epoch: 2/10... Step: 3680... Loss: 3.7322... ppl: 38.7311  Val Loss: 3.6566\n",
      "Epoch: 2/10... Step: 3712... Loss: 3.5348... ppl: 38.4541  Val Loss: 3.6495\n",
      "Epoch: 2/10... Step: 3744... Loss: 3.4433... ppl: 38.2584  Val Loss: 3.6444\n",
      "Epoch: 2/10... Step: 3776... Loss: 3.9100... ppl: 38.3711  Val Loss: 3.6473\n",
      "Epoch: 2/10... Step: 3808... Loss: 5.0148... ppl: 38.1552  Val Loss: 3.6417\n",
      "Epoch: 2/10... Step: 3840... Loss: 4.3410... ppl: 38.3197  Val Loss: 3.6460\n",
      "Epoch: 2/10... Step: 3872... Loss: 3.9629... ppl: 37.9760  Val Loss: 3.6370\n",
      "Epoch: 2/10... Step: 3904... Loss: 2.9083... ppl: 37.8593  Val Loss: 3.6339\n",
      "Epoch: 2/10... Step: 3936... Loss: 3.7850... ppl: 37.9052  Val Loss: 3.6351\n",
      "Epoch: 2/10... Step: 3968... Loss: 3.9118... ppl: 37.5673  Val Loss: 3.6261\n",
      "Epoch: 2/10... Step: 4000... Loss: 3.7466... ppl: 37.5269  Val Loss: 3.6251\n",
      "Epoch: 2/10... Step: 4032... Loss: 4.1994... ppl: 37.5235  Val Loss: 3.6250\n",
      "Epoch: 2/10... Step: 4064... Loss: 3.6449... ppl: 37.3825  Val Loss: 3.6212\n",
      "Epoch: 2/10... Step: 4096... Loss: 3.5509... ppl: 37.3515  Val Loss: 3.6204\n",
      "Epoch: 2/10... Step: 4128... Loss: 2.9240... ppl: 37.3479  Val Loss: 3.6203\n",
      "Epoch: 2/10... Step: 4160... Loss: 2.9851... ppl: 37.4135  Val Loss: 3.6220\n",
      "Epoch: 2/10... Step: 4192... Loss: 3.4879... ppl: 37.2095  Val Loss: 3.6166\n",
      "Epoch: 2/10... Step: 4224... Loss: 3.3696... ppl: 36.9314  Val Loss: 3.6091\n",
      "Epoch: 2/10... Step: 4256... Loss: 3.4362... ppl: 37.0423  Val Loss: 3.6121\n",
      "Epoch: 2/10... Step: 4288... Loss: 3.4145... ppl: 36.7652  Val Loss: 3.6046\n",
      "Epoch: 2/10... Step: 4320... Loss: 2.9739... ppl: 36.7940  Val Loss: 3.6053\n",
      "Epoch: 2/10... Step: 4352... Loss: 2.8879... ppl: 36.8388  Val Loss: 3.6066\n",
      "Epoch: 2/10... Step: 4384... Loss: 3.1406... ppl: 36.5349  Val Loss: 3.5983\n",
      "Epoch: 2/10... Step: 4416... Loss: 3.9196... ppl: 36.5073  Val Loss: 3.5975\n",
      "Epoch: 2/10... Step: 4448... Loss: 2.9335... ppl: 36.6507  Val Loss: 3.6014\n",
      "Epoch: 2/10... Step: 4480... Loss: 4.2965... ppl: 36.5525  Val Loss: 3.5988\n",
      "Epoch: 2/10... Step: 4512... Loss: 3.2716... ppl: 36.3309  Val Loss: 3.5927\n",
      "Epoch: 2/10... Step: 4544... Loss: 3.4402... ppl: 36.3140  Val Loss: 3.5922\n",
      "Epoch: 2/10... Step: 4576... Loss: 2.7276... ppl: 36.4007  Val Loss: 3.5946\n",
      "Epoch: 2/10... Step: 4608... Loss: 2.9312... ppl: 36.2971  Val Loss: 3.5917\n",
      "Epoch: 2/10... Step: 4640... Loss: 3.6646... ppl: 36.0974  Val Loss: 3.5862\n",
      "Epoch: 2/10... Step: 4672... Loss: 3.5219... ppl: 36.0256  Val Loss: 3.5842\n",
      "Epoch: 3/10... Step: 4704... Loss: 3.1279... ppl: 36.0517  Val Loss: 3.5850\n",
      "Epoch: 3/10... Step: 4736... Loss: 4.2049... ppl: 36.2754  Val Loss: 3.5911\n",
      "Epoch: 3/10... Step: 4768... Loss: 3.1158... ppl: 36.1801  Val Loss: 3.5885\n",
      "Epoch: 3/10... Step: 4800... Loss: 2.6573... ppl: 36.2049  Val Loss: 3.5892\n",
      "Epoch: 3/10... Step: 4832... Loss: 3.3160... ppl: 36.1336  Val Loss: 3.5872\n",
      "Epoch: 3/10... Step: 4864... Loss: 3.7040... ppl: 36.0094  Val Loss: 3.5838\n",
      "Epoch: 3/10... Step: 4896... Loss: 2.5075... ppl: 36.0117  Val Loss: 3.5838\n",
      "Epoch: 3/10... Step: 4928... Loss: 2.1054... ppl: 35.9883  Val Loss: 3.5832\n",
      "Epoch: 3/10... Step: 4960... Loss: 3.3345... ppl: 35.9734  Val Loss: 3.5828\n",
      "Epoch: 3/10... Step: 4992... Loss: 3.4648... ppl: 36.3293  Val Loss: 3.5926\n",
      "Epoch: 3/10... Step: 5024... Loss: 3.4312... ppl: 35.9414  Val Loss: 3.5819\n",
      "Epoch: 3/10... Step: 5056... Loss: 3.1010... ppl: 35.7641  Val Loss: 3.5769\n",
      "Epoch: 3/10... Step: 5088... Loss: 2.7943... ppl: 35.7397  Val Loss: 3.5763\n",
      "Epoch: 3/10... Step: 5120... Loss: 3.8163... ppl: 35.9422  Val Loss: 3.5819\n",
      "Epoch: 3/10... Step: 5152... Loss: 2.8994... ppl: 35.9725  Val Loss: 3.5828\n",
      "Epoch: 3/10... Step: 5184... Loss: 2.8602... ppl: 35.9267  Val Loss: 3.5815\n",
      "Epoch: 3/10... Step: 5216... Loss: 2.9776... ppl: 35.5679  Val Loss: 3.5714\n",
      "Epoch: 3/10... Step: 5248... Loss: 3.0194... ppl: 35.5829  Val Loss: 3.5719\n",
      "Epoch: 3/10... Step: 5280... Loss: 3.2796... ppl: 35.8381  Val Loss: 3.5790\n",
      "Epoch: 3/10... Step: 5312... Loss: 3.4494... ppl: 35.6533  Val Loss: 3.5738\n",
      "Epoch: 3/10... Step: 5344... Loss: 3.6658... ppl: 35.6178  Val Loss: 3.5728\n",
      "Epoch: 3/10... Step: 5376... Loss: 4.0757... ppl: 35.5468  Val Loss: 3.5708\n",
      "Epoch: 3/10... Step: 5408... Loss: 3.8149... ppl: 35.5010  Val Loss: 3.5696\n",
      "Epoch: 3/10... Step: 5440... Loss: 4.3804... ppl: 35.5088  Val Loss: 3.5698\n",
      "Epoch: 3/10... Step: 5472... Loss: 3.7281... ppl: 35.2090  Val Loss: 3.5613\n",
      "Epoch: 3/10... Step: 5504... Loss: 3.6217... ppl: 35.3745  Val Loss: 3.5660\n",
      "Epoch: 3/10... Step: 5536... Loss: 3.2480... ppl: 35.4467  Val Loss: 3.5680\n",
      "Epoch: 3/10... Step: 5568... Loss: 3.0417... ppl: 35.1970  Val Loss: 3.5610\n",
      "Epoch: 3/10... Step: 5600... Loss: 3.5482... ppl: 35.1603  Val Loss: 3.5599\n",
      "Epoch: 3/10... Step: 5632... Loss: 3.3674... ppl: 35.1637  Val Loss: 3.5600\n",
      "Epoch: 3/10... Step: 5664... Loss: 3.2384... ppl: 35.1976  Val Loss: 3.5610\n",
      "Epoch: 3/10... Step: 5696... Loss: 3.7263... ppl: 34.9820  Val Loss: 3.5548\n",
      "Epoch: 3/10... Step: 5728... Loss: 3.1947... ppl: 35.4312  Val Loss: 3.5676\n",
      "Epoch: 3/10... Step: 5760... Loss: 3.8265... ppl: 35.1332  Val Loss: 3.5591\n",
      "Epoch: 3/10... Step: 5792... Loss: 3.9673... ppl: 34.9461  Val Loss: 3.5538\n",
      "Epoch: 3/10... Step: 5824... Loss: 3.2149... ppl: 35.0934  Val Loss: 3.5580\n",
      "Epoch: 3/10... Step: 5856... Loss: 3.3830... ppl: 34.7986  Val Loss: 3.5496\n",
      "Epoch: 3/10... Step: 5888... Loss: 3.0997... ppl: 34.8782  Val Loss: 3.5519\n",
      "Epoch: 3/10... Step: 5920... Loss: 3.5999... ppl: 34.7652  Val Loss: 3.5486\n",
      "Epoch: 3/10... Step: 5952... Loss: 3.8449... ppl: 34.6675  Val Loss: 3.5458\n",
      "Epoch: 3/10... Step: 5984... Loss: 3.2659... ppl: 34.8045  Val Loss: 3.5497\n",
      "Epoch: 3/10... Step: 6016... Loss: 3.8039... ppl: 34.7483  Val Loss: 3.5481\n",
      "Epoch: 3/10... Step: 6048... Loss: 3.4162... ppl: 35.0102  Val Loss: 3.5556\n",
      "Epoch: 3/10... Step: 6080... Loss: 2.7609... ppl: 34.6858  Val Loss: 3.5463\n",
      "Epoch: 3/10... Step: 6112... Loss: 3.2210... ppl: 34.7091  Val Loss: 3.5470\n",
      "Epoch: 3/10... Step: 6144... Loss: 3.1625... ppl: 34.5952  Val Loss: 3.5437\n",
      "Epoch: 3/10... Step: 6176... Loss: 3.1546... ppl: 34.6147  Val Loss: 3.5443\n",
      "Epoch: 3/10... Step: 6208... Loss: 3.3724... ppl: 34.5688  Val Loss: 3.5430\n",
      "Epoch: 3/10... Step: 6240... Loss: 2.6968... ppl: 34.5363  Val Loss: 3.5420\n",
      "Epoch: 3/10... Step: 6272... Loss: 2.7251... ppl: 34.6732  Val Loss: 3.5460\n",
      "Epoch: 3/10... Step: 6304... Loss: 3.1263... ppl: 34.3910  Val Loss: 3.5378\n",
      "Epoch: 3/10... Step: 6336... Loss: 3.4929... ppl: 34.3030  Val Loss: 3.5352\n",
      "Epoch: 3/10... Step: 6368... Loss: 3.1422... ppl: 34.3861  Val Loss: 3.5377\n",
      "Epoch: 3/10... Step: 6400... Loss: 3.5310... ppl: 34.3462  Val Loss: 3.5365\n",
      "Epoch: 3/10... Step: 6432... Loss: 3.6780... ppl: 34.3071  Val Loss: 3.5354\n",
      "Epoch: 3/10... Step: 6464... Loss: 3.6946... ppl: 34.4397  Val Loss: 3.5392\n",
      "Epoch: 3/10... Step: 6496... Loss: 2.7927... ppl: 34.2590  Val Loss: 3.5339\n",
      "Epoch: 3/10... Step: 6528... Loss: 3.1358... ppl: 34.3964  Val Loss: 3.5380\n",
      "Epoch: 3/10... Step: 6560... Loss: 3.3713... ppl: 34.2559  Val Loss: 3.5339\n",
      "Epoch: 3/10... Step: 6592... Loss: 2.5810... ppl: 34.3701  Val Loss: 3.5372\n",
      "Epoch: 3/10... Step: 6624... Loss: 3.6542... ppl: 34.1103  Val Loss: 3.5296\n",
      "Epoch: 3/10... Step: 6656... Loss: 3.0364... ppl: 34.0844  Val Loss: 3.5288\n",
      "Epoch: 3/10... Step: 6688... Loss: 2.8273... ppl: 34.2753  Val Loss: 3.5344\n",
      "Epoch: 3/10... Step: 6720... Loss: 2.4828... ppl: 33.9982  Val Loss: 3.5263\n",
      "Epoch: 3/10... Step: 6752... Loss: 3.4885... ppl: 33.9235  Val Loss: 3.5241\n",
      "Epoch: 3/10... Step: 6784... Loss: 3.1361... ppl: 34.0543  Val Loss: 3.5280\n",
      "Epoch: 3/10... Step: 6816... Loss: 2.7266... ppl: 34.1287  Val Loss: 3.5301\n",
      "Epoch: 3/10... Step: 6848... Loss: 2.9003... ppl: 33.8112  Val Loss: 3.5208\n",
      "Epoch: 3/10... Step: 6880... Loss: 3.0331... ppl: 33.9016  Val Loss: 3.5235\n",
      "Epoch: 3/10... Step: 6912... Loss: 4.0770... ppl: 34.0201  Val Loss: 3.5270\n",
      "Epoch: 3/10... Step: 6944... Loss: 3.2191... ppl: 34.2066  Val Loss: 3.5324\n",
      "Epoch: 3/10... Step: 6976... Loss: 3.4890... ppl: 33.8589  Val Loss: 3.5222\n",
      "Epoch: 3/10... Step: 7008... Loss: 3.3698... ppl: 33.8407  Val Loss: 3.5217\n",
      "Epoch: 4/10... Step: 7040... Loss: 3.5654... ppl: 33.7481  Val Loss: 3.5189\n",
      "Epoch: 4/10... Step: 7072... Loss: 2.9746... ppl: 34.2504  Val Loss: 3.5337\n",
      "Epoch: 4/10... Step: 7104... Loss: 3.3213... ppl: 33.8931  Val Loss: 3.5232\n",
      "Epoch: 4/10... Step: 7136... Loss: 2.8216... ppl: 34.0040  Val Loss: 3.5265\n",
      "Epoch: 4/10... Step: 7168... Loss: 3.4647... ppl: 34.0616  Val Loss: 3.5282\n",
      "Epoch: 4/10... Step: 7200... Loss: 2.6694... ppl: 33.9475  Val Loss: 3.5248\n",
      "Epoch: 4/10... Step: 7232... Loss: 3.0566... ppl: 34.0883  Val Loss: 3.5290\n",
      "Epoch: 4/10... Step: 7264... Loss: 4.1419... ppl: 33.9969  Val Loss: 3.5263\n",
      "Epoch: 4/10... Step: 7296... Loss: 3.5846... ppl: 34.0443  Val Loss: 3.5277\n",
      "Epoch: 4/10... Step: 7328... Loss: 2.5779... ppl: 34.3360  Val Loss: 3.5362\n",
      "Epoch: 4/10... Step: 7360... Loss: 2.9130... ppl: 34.0933  Val Loss: 3.5291\n",
      "Epoch: 4/10... Step: 7392... Loss: 3.2296... ppl: 33.9751  Val Loss: 3.5256\n",
      "Epoch: 4/10... Step: 7424... Loss: 2.7401... ppl: 33.7085  Val Loss: 3.5178\n",
      "Epoch: 4/10... Step: 7456... Loss: 2.6941... ppl: 33.9947  Val Loss: 3.5262\n",
      "Epoch: 4/10... Step: 7488... Loss: 3.4384... ppl: 34.0868  Val Loss: 3.5289\n",
      "Epoch: 4/10... Step: 7520... Loss: 2.0073... ppl: 34.2576  Val Loss: 3.5339\n",
      "Epoch: 4/10... Step: 7552... Loss: 3.2505... ppl: 34.0036  Val Loss: 3.5265\n",
      "Epoch: 4/10... Step: 7584... Loss: 2.6687... ppl: 34.0743  Val Loss: 3.5285\n",
      "Epoch: 4/10... Step: 7616... Loss: 3.0823... ppl: 34.1898  Val Loss: 3.5319\n",
      "Epoch: 4/10... Step: 7648... Loss: 3.4143... ppl: 34.0778  Val Loss: 3.5286\n",
      "Epoch: 4/10... Step: 7680... Loss: 2.9386... ppl: 34.0622  Val Loss: 3.5282\n",
      "Epoch: 4/10... Step: 7712... Loss: 2.7890... ppl: 34.0084  Val Loss: 3.5266\n",
      "Epoch: 4/10... Step: 7744... Loss: 3.3860... ppl: 34.0160  Val Loss: 3.5268\n",
      "Epoch: 4/10... Step: 7776... Loss: 3.6854... ppl: 34.0462  Val Loss: 3.5277\n",
      "Epoch: 4/10... Step: 7808... Loss: 3.5561... ppl: 33.7972  Val Loss: 3.5204\n",
      "Epoch: 4/10... Step: 7840... Loss: 2.1241... ppl: 33.9933  Val Loss: 3.5262\n",
      "Epoch: 4/10... Step: 7872... Loss: 3.6668... ppl: 34.0306  Val Loss: 3.5273\n",
      "Epoch: 4/10... Step: 7904... Loss: 2.2593... ppl: 33.9059  Val Loss: 3.5236\n",
      "Epoch: 4/10... Step: 7936... Loss: 2.9964... ppl: 33.8687  Val Loss: 3.5225\n",
      "Epoch: 4/10... Step: 7968... Loss: 3.7888... ppl: 33.7690  Val Loss: 3.5195\n",
      "Epoch: 4/10... Step: 8000... Loss: 2.9844... ppl: 33.9318  Val Loss: 3.5244\n",
      "Epoch: 4/10... Step: 8032... Loss: 3.2718... ppl: 33.7959  Val Loss: 3.5203\n",
      "Epoch: 4/10... Step: 8064... Loss: 2.4267... ppl: 33.9470  Val Loss: 3.5248\n",
      "Epoch: 4/10... Step: 8096... Loss: 3.0392... ppl: 33.9893  Val Loss: 3.5260\n",
      "Epoch: 4/10... Step: 8128... Loss: 2.3286... ppl: 33.7520  Val Loss: 3.5190\n",
      "Epoch: 4/10... Step: 8160... Loss: 2.9562... ppl: 33.7997  Val Loss: 3.5205\n",
      "Epoch: 4/10... Step: 8192... Loss: 3.2740... ppl: 33.7412  Val Loss: 3.5187\n",
      "Epoch: 4/10... Step: 8224... Loss: 3.0451... ppl: 33.6656  Val Loss: 3.5165\n",
      "Epoch: 4/10... Step: 8256... Loss: 2.6617... ppl: 33.6108  Val Loss: 3.5148\n",
      "Epoch: 4/10... Step: 8288... Loss: 3.1201... ppl: 33.5910  Val Loss: 3.5143\n",
      "Epoch: 4/10... Step: 8320... Loss: 3.1972... ppl: 33.5589  Val Loss: 3.5133\n",
      "Epoch: 4/10... Step: 8352... Loss: 2.8096... ppl: 33.5774  Val Loss: 3.5139\n",
      "Epoch: 4/10... Step: 8384... Loss: 2.6878... ppl: 33.9805  Val Loss: 3.5258\n",
      "Epoch: 4/10... Step: 8416... Loss: 2.7680... ppl: 33.7171  Val Loss: 3.5180\n",
      "Epoch: 4/10... Step: 8448... Loss: 4.0708... ppl: 33.6660  Val Loss: 3.5165\n",
      "Epoch: 4/10... Step: 8480... Loss: 2.8184... ppl: 33.7573  Val Loss: 3.5192\n",
      "Epoch: 4/10... Step: 8512... Loss: 2.4355... ppl: 33.7092  Val Loss: 3.5178\n",
      "Epoch: 4/10... Step: 8544... Loss: 3.1442... ppl: 33.7535  Val Loss: 3.5191\n",
      "Epoch: 4/10... Step: 8576... Loss: 2.9543... ppl: 33.7077  Val Loss: 3.5177\n",
      "Epoch: 4/10... Step: 8608... Loss: 3.4174... ppl: 33.7279  Val Loss: 3.5183\n",
      "Epoch: 4/10... Step: 8640... Loss: 3.2294... ppl: 33.6401  Val Loss: 3.5157\n",
      "Epoch: 4/10... Step: 8672... Loss: 2.6934... ppl: 33.5009  Val Loss: 3.5116\n",
      "Epoch: 4/10... Step: 8704... Loss: 3.7898... ppl: 33.4934  Val Loss: 3.5113\n",
      "Epoch: 4/10... Step: 8736... Loss: 3.0070... ppl: 33.5350  Val Loss: 3.5126\n",
      "Epoch: 4/10... Step: 8768... Loss: 2.7105... ppl: 33.5882  Val Loss: 3.5142\n",
      "Epoch: 4/10... Step: 8800... Loss: 3.4702... ppl: 33.6374  Val Loss: 3.5156\n",
      "Epoch: 4/10... Step: 8832... Loss: 2.9958... ppl: 33.5187  Val Loss: 3.5121\n",
      "Epoch: 4/10... Step: 8864... Loss: 3.2068... ppl: 33.7204  Val Loss: 3.5181\n",
      "Epoch: 4/10... Step: 8896... Loss: 2.6869... ppl: 33.6494  Val Loss: 3.5160\n",
      "Epoch: 4/10... Step: 8928... Loss: 2.8496... ppl: 33.6395  Val Loss: 3.5157\n",
      "Epoch: 4/10... Step: 8960... Loss: 2.4276... ppl: 33.4736  Val Loss: 3.5108\n",
      "Epoch: 4/10... Step: 8992... Loss: 3.0350... ppl: 33.5027  Val Loss: 3.5116\n",
      "Epoch: 4/10... Step: 9024... Loss: 3.3974... ppl: 33.6964  Val Loss: 3.5174\n",
      "Epoch: 4/10... Step: 9056... Loss: 3.0911... ppl: 33.4652  Val Loss: 3.5105\n",
      "Epoch: 4/10... Step: 9088... Loss: 3.4126... ppl: 33.4070  Val Loss: 3.5088\n",
      "Epoch: 4/10... Step: 9120... Loss: 2.6680... ppl: 33.4972  Val Loss: 3.5115\n",
      "Epoch: 4/10... Step: 9152... Loss: 2.8928... ppl: 33.6067  Val Loss: 3.5147\n",
      "Epoch: 4/10... Step: 9184... Loss: 3.3136... ppl: 33.3511  Val Loss: 3.5071\n",
      "Epoch: 4/10... Step: 9216... Loss: 2.3939... ppl: 33.3372  Val Loss: 3.5067\n",
      "Epoch: 4/10... Step: 9248... Loss: 3.6884... ppl: 33.4248  Val Loss: 3.5093\n",
      "Epoch: 4/10... Step: 9280... Loss: 2.6926... ppl: 34.0423  Val Loss: 3.5276\n",
      "Epoch: 4/10... Step: 9312... Loss: 3.4680... ppl: 33.4454  Val Loss: 3.5099\n",
      "Epoch: 4/10... Step: 9344... Loss: 3.2985... ppl: 33.3470  Val Loss: 3.5070\n",
      "Epoch: 5/10... Step: 9376... Loss: 1.9438... ppl: 33.2811  Val Loss: 3.5050\n",
      "Epoch: 5/10... Step: 9408... Loss: 3.2765... ppl: 33.6005  Val Loss: 3.5145\n",
      "Epoch: 5/10... Step: 9440... Loss: 2.8867... ppl: 33.3709  Val Loss: 3.5077\n",
      "Epoch: 5/10... Step: 9472... Loss: 2.6943... ppl: 33.4836  Val Loss: 3.5111\n",
      "Epoch: 5/10... Step: 9504... Loss: 3.0341... ppl: 33.5790  Val Loss: 3.5139\n",
      "Epoch: 5/10... Step: 9536... Loss: 2.2137... ppl: 33.4832  Val Loss: 3.5110\n",
      "Epoch: 5/10... Step: 9568... Loss: 1.8845... ppl: 33.7100  Val Loss: 3.5178\n",
      "Epoch: 5/10... Step: 9600... Loss: 3.1664... ppl: 33.5851  Val Loss: 3.5141\n",
      "Epoch: 5/10... Step: 9632... Loss: 2.9756... ppl: 33.6850  Val Loss: 3.5171\n",
      "Epoch: 5/10... Step: 9664... Loss: 3.2524... ppl: 33.8617  Val Loss: 3.5223\n",
      "Epoch: 5/10... Step: 9696... Loss: 2.2618... ppl: 33.9975  Val Loss: 3.5263\n",
      "Epoch: 5/10... Step: 9728... Loss: 3.2601... ppl: 33.6961  Val Loss: 3.5174\n",
      "Epoch: 5/10... Step: 9760... Loss: 2.4250... ppl: 33.4598  Val Loss: 3.5103\n",
      "Epoch: 5/10... Step: 9792... Loss: 3.3519... ppl: 33.5731  Val Loss: 3.5137\n",
      "Epoch: 5/10... Step: 9824... Loss: 3.3491... ppl: 33.8768  Val Loss: 3.5227\n",
      "Epoch: 5/10... Step: 9856... Loss: 2.0366... ppl: 34.0770  Val Loss: 3.5286\n",
      "Epoch: 5/10... Step: 9888... Loss: 3.1800... ppl: 33.9062  Val Loss: 3.5236\n",
      "Epoch: 5/10... Step: 9920... Loss: 2.9122... ppl: 33.9170  Val Loss: 3.5239\n",
      "Epoch: 5/10... Step: 9952... Loss: 3.2824... ppl: 33.9472  Val Loss: 3.5248\n",
      "Epoch: 5/10... Step: 9984... Loss: 3.1112... ppl: 34.0550  Val Loss: 3.5280\n",
      "Epoch: 5/10... Step: 10016... Loss: 2.7870... ppl: 34.0804  Val Loss: 3.5287\n",
      "Epoch: 5/10... Step: 10048... Loss: 2.7709... ppl: 33.9717  Val Loss: 3.5255\n",
      "Epoch: 5/10... Step: 10080... Loss: 2.8281... ppl: 34.0110  Val Loss: 3.5267\n",
      "Epoch: 5/10... Step: 10112... Loss: 3.7726... ppl: 34.0416  Val Loss: 3.5276\n",
      "Epoch: 5/10... Step: 10144... Loss: 2.7836... ppl: 33.7804  Val Loss: 3.5199\n",
      "Epoch: 5/10... Step: 10176... Loss: 2.4899... ppl: 33.9976  Val Loss: 3.5263\n",
      "Epoch: 5/10... Step: 10208... Loss: 3.3556... ppl: 33.9776  Val Loss: 3.5257\n",
      "Epoch: 5/10... Step: 10240... Loss: 3.0743... ppl: 33.9860  Val Loss: 3.5259\n",
      "Epoch: 5/10... Step: 10272... Loss: 3.6407... ppl: 33.8419  Val Loss: 3.5217\n",
      "Epoch: 5/10... Step: 10304... Loss: 3.7038... ppl: 33.5702  Val Loss: 3.5136\n",
      "Epoch: 5/10... Step: 10336... Loss: 2.9079... ppl: 33.7741  Val Loss: 3.5197\n",
      "Epoch: 5/10... Step: 10368... Loss: 2.9171... ppl: 33.7185  Val Loss: 3.5180\n",
      "Epoch: 5/10... Step: 10400... Loss: 3.0359... ppl: 33.6346  Val Loss: 3.5156\n",
      "Epoch: 5/10... Step: 10432... Loss: 2.9324... ppl: 34.0101  Val Loss: 3.5267\n",
      "Epoch: 5/10... Step: 10464... Loss: 3.1057... ppl: 33.7398  Val Loss: 3.5187\n",
      "Epoch: 5/10... Step: 10496... Loss: 2.5421... ppl: 33.7367  Val Loss: 3.5186\n",
      "Epoch: 5/10... Step: 10528... Loss: 3.2578... ppl: 33.7885  Val Loss: 3.5201\n",
      "Epoch: 5/10... Step: 10560... Loss: 3.5493... ppl: 33.5487  Val Loss: 3.5130\n",
      "Epoch: 5/10... Step: 10592... Loss: 2.9203... ppl: 33.6195  Val Loss: 3.5151\n",
      "Epoch: 5/10... Step: 10624... Loss: 2.9510... ppl: 33.6815  Val Loss: 3.5169\n",
      "Epoch: 5/10... Step: 10656... Loss: 2.9699... ppl: 33.5653  Val Loss: 3.5135\n",
      "Epoch: 5/10... Step: 10688... Loss: 2.8544... ppl: 33.5983  Val Loss: 3.5145\n",
      "Epoch: 5/10... Step: 10720... Loss: 2.8049... ppl: 33.8747  Val Loss: 3.5227\n",
      "Epoch: 5/10... Step: 10752... Loss: 2.6391... ppl: 33.9299  Val Loss: 3.5243\n",
      "Epoch: 5/10... Step: 10784... Loss: 3.5400... ppl: 33.8041  Val Loss: 3.5206\n",
      "Epoch: 5/10... Step: 10816... Loss: 3.1213... ppl: 33.9602  Val Loss: 3.5252\n",
      "Epoch: 5/10... Step: 10848... Loss: 3.3889... ppl: 33.8566  Val Loss: 3.5221\n",
      "Epoch: 5/10... Step: 10880... Loss: 2.7753... ppl: 33.8369  Val Loss: 3.5216\n",
      "Epoch: 5/10... Step: 10912... Loss: 2.4002... ppl: 33.7615  Val Loss: 3.5193\n",
      "Epoch: 5/10... Step: 10944... Loss: 3.1356... ppl: 33.7735  Val Loss: 3.5197\n",
      "Epoch: 5/10... Step: 10976... Loss: 3.1945... ppl: 33.7983  Val Loss: 3.5204\n",
      "Epoch: 5/10... Step: 11008... Loss: 3.1241... ppl: 33.6228  Val Loss: 3.5152\n",
      "Epoch: 5/10... Step: 11040... Loss: 3.4899... ppl: 33.5819  Val Loss: 3.5140\n",
      "Epoch: 5/10... Step: 11072... Loss: 3.4802... ppl: 33.7163  Val Loss: 3.5180\n",
      "Epoch: 5/10... Step: 11104... Loss: 3.0314... ppl: 33.7812  Val Loss: 3.5199\n",
      "Epoch: 5/10... Step: 11136... Loss: 3.5866... ppl: 33.7675  Val Loss: 3.5195\n",
      "Epoch: 5/10... Step: 11168... Loss: 2.3649... ppl: 33.7438  Val Loss: 3.5188\n",
      "Epoch: 5/10... Step: 11200... Loss: 2.8620... ppl: 33.9254  Val Loss: 3.5242\n",
      "Epoch: 5/10... Step: 11232... Loss: 2.7374... ppl: 33.8290  Val Loss: 3.5213\n",
      "Epoch: 5/10... Step: 11264... Loss: 2.0612... ppl: 33.8184  Val Loss: 3.5210\n",
      "Epoch: 5/10... Step: 11296... Loss: 2.6907... ppl: 33.8068  Val Loss: 3.5207\n",
      "Epoch: 5/10... Step: 11328... Loss: 3.4930... ppl: 33.6894  Val Loss: 3.5172\n",
      "Epoch: 5/10... Step: 11360... Loss: 2.7942... ppl: 33.8969  Val Loss: 3.5233\n",
      "Epoch: 5/10... Step: 11392... Loss: 2.5709... ppl: 33.7545  Val Loss: 3.5191\n",
      "Epoch: 5/10... Step: 11424... Loss: 3.4372... ppl: 33.5812  Val Loss: 3.5140\n",
      "Epoch: 5/10... Step: 11456... Loss: 2.1946... ppl: 33.6482  Val Loss: 3.5160\n",
      "Epoch: 5/10... Step: 11488... Loss: 2.7612... ppl: 33.7981  Val Loss: 3.5204\n",
      "Epoch: 5/10... Step: 11520... Loss: 2.3256... ppl: 33.6354  Val Loss: 3.5156\n",
      "Epoch: 5/10... Step: 11552... Loss: 3.6497... ppl: 33.5995  Val Loss: 3.5145\n",
      "Epoch: 5/10... Step: 11584... Loss: 2.9411... ppl: 33.6593  Val Loss: 3.5163\n",
      "Epoch: 5/10... Step: 11616... Loss: 2.3266... ppl: 34.2371  Val Loss: 3.5333\n",
      "Epoch: 5/10... Step: 11648... Loss: 2.7930... ppl: 33.8488  Val Loss: 3.5219\n",
      "Epoch: 5/10... Step: 11680... Loss: 3.2792... ppl: 33.5776  Val Loss: 3.5139\n",
      "Epoch: 5/10... Step: 11712... Loss: 3.1150... ppl: 33.6029  Val Loss: 3.5146\n",
      "Epoch: 6/10... Step: 11744... Loss: 3.4628... ppl: 33.6720  Val Loss: 3.5167\n",
      "Epoch: 6/10... Step: 11776... Loss: 2.8552... ppl: 33.7234  Val Loss: 3.5182\n",
      "Epoch: 6/10... Step: 11808... Loss: 2.6094... ppl: 33.6899  Val Loss: 3.5172\n",
      "Epoch: 6/10... Step: 11840... Loss: 3.3010... ppl: 33.7712  Val Loss: 3.5196\n",
      "Epoch: 6/10... Step: 11872... Loss: 2.9250... ppl: 33.6356  Val Loss: 3.5156\n",
      "Epoch: 6/10... Step: 11904... Loss: 2.9757... ppl: 33.8367  Val Loss: 3.5215\n",
      "Epoch: 6/10... Step: 11936... Loss: 2.9794... ppl: 33.8993  Val Loss: 3.5234\n",
      "Epoch: 6/10... Step: 11968... Loss: 2.7282... ppl: 33.9932  Val Loss: 3.5262\n",
      "Epoch: 6/10... Step: 12000... Loss: 2.4723... ppl: 33.9943  Val Loss: 3.5262\n",
      "Epoch: 6/10... Step: 12032... Loss: 2.9416... ppl: 34.4080  Val Loss: 3.5383\n",
      "Epoch: 6/10... Step: 12064... Loss: 3.0627... ppl: 33.9769  Val Loss: 3.5257\n",
      "Epoch: 6/10... Step: 12096... Loss: 3.8096... ppl: 33.7640  Val Loss: 3.5194\n",
      "Epoch: 6/10... Step: 12128... Loss: 2.5326... ppl: 33.8784  Val Loss: 3.5228\n",
      "Epoch: 6/10... Step: 12160... Loss: 3.0443... ppl: 34.1606  Val Loss: 3.5311\n",
      "Epoch: 6/10... Step: 12192... Loss: 2.7992... ppl: 34.4885  Val Loss: 3.5406\n",
      "Epoch: 6/10... Step: 12224... Loss: 2.6065... ppl: 34.4172  Val Loss: 3.5386\n",
      "Epoch: 6/10... Step: 12256... Loss: 2.6973... ppl: 34.3912  Val Loss: 3.5378\n",
      "Epoch: 6/10... Step: 12288... Loss: 2.7770... ppl: 34.3596  Val Loss: 3.5369\n",
      "Epoch: 6/10... Step: 12320... Loss: 2.8429... ppl: 34.5388  Val Loss: 3.5421\n",
      "Epoch: 6/10... Step: 12352... Loss: 2.7066... ppl: 34.5650  Val Loss: 3.5428\n",
      "Epoch: 6/10... Step: 12384... Loss: 3.0813... ppl: 34.5147  Val Loss: 3.5414\n",
      "Epoch: 6/10... Step: 12416... Loss: 3.0299... ppl: 34.5278  Val Loss: 3.5418\n",
      "Epoch: 6/10... Step: 12448... Loss: 3.0130... ppl: 34.4869  Val Loss: 3.5406\n",
      "Epoch: 6/10... Step: 12480... Loss: 2.5678... ppl: 34.3515  Val Loss: 3.5366\n",
      "Epoch: 6/10... Step: 12512... Loss: 2.8080... ppl: 34.3531  Val Loss: 3.5367\n",
      "Epoch: 6/10... Step: 12544... Loss: 2.8982... ppl: 34.4589  Val Loss: 3.5398\n",
      "Epoch: 6/10... Step: 12576... Loss: 2.4371... ppl: 34.5037  Val Loss: 3.5411\n",
      "Epoch: 6/10... Step: 12608... Loss: 3.1717... ppl: 34.3458  Val Loss: 3.5365\n",
      "Epoch: 6/10... Step: 12640... Loss: 2.5300... ppl: 34.0742  Val Loss: 3.5285\n",
      "Epoch: 6/10... Step: 12672... Loss: 3.0506... ppl: 34.2068  Val Loss: 3.5324\n",
      "Epoch: 6/10... Step: 12704... Loss: 2.5471... ppl: 34.1878  Val Loss: 3.5319\n",
      "Epoch: 6/10... Step: 12736... Loss: 3.3362... ppl: 34.0284  Val Loss: 3.5272\n",
      "Epoch: 6/10... Step: 12768... Loss: 2.5439... ppl: 34.5194  Val Loss: 3.5415\n",
      "Epoch: 6/10... Step: 12800... Loss: 2.9131... ppl: 34.3918  Val Loss: 3.5378\n",
      "Epoch: 6/10... Step: 12832... Loss: 2.8625... ppl: 34.2744  Val Loss: 3.5344\n",
      "Epoch: 6/10... Step: 12864... Loss: 2.9369... ppl: 34.3349  Val Loss: 3.5362\n",
      "Epoch: 6/10... Step: 12896... Loss: 3.3246... ppl: 34.0888  Val Loss: 3.5290\n",
      "Epoch: 6/10... Step: 12928... Loss: 2.7637... ppl: 34.1281  Val Loss: 3.5301\n",
      "Epoch: 6/10... Step: 12960... Loss: 2.9552... ppl: 34.0972  Val Loss: 3.5292\n",
      "Epoch: 6/10... Step: 12992... Loss: 3.0640... ppl: 34.1025  Val Loss: 3.5294\n",
      "Epoch: 6/10... Step: 13024... Loss: 2.3630... ppl: 34.1646  Val Loss: 3.5312\n",
      "Epoch: 6/10... Step: 13056... Loss: 2.5557... ppl: 34.2588  Val Loss: 3.5339\n",
      "Epoch: 6/10... Step: 13088... Loss: 2.6339... ppl: 34.5720  Val Loss: 3.5430\n",
      "Epoch: 6/10... Step: 13120... Loss: 3.2181... ppl: 34.3069  Val Loss: 3.5353\n",
      "Epoch: 6/10... Step: 13152... Loss: 2.6953... ppl: 34.5297  Val Loss: 3.5418\n",
      "Epoch: 6/10... Step: 13184... Loss: 3.2822... ppl: 34.4071  Val Loss: 3.5383\n",
      "Epoch: 6/10... Step: 13216... Loss: 3.3452... ppl: 34.2976  Val Loss: 3.5351\n",
      "Epoch: 6/10... Step: 13248... Loss: 3.2945... ppl: 34.3412  Val Loss: 3.5363\n",
      "Epoch: 6/10... Step: 13280... Loss: 3.2483... ppl: 34.2995  Val Loss: 3.5351\n",
      "Epoch: 6/10... Step: 13312... Loss: 2.8162... ppl: 34.3493  Val Loss: 3.5366\n",
      "Epoch: 6/10... Step: 13344... Loss: 2.4425... ppl: 34.1956  Val Loss: 3.5321\n",
      "Epoch: 6/10... Step: 13376... Loss: 2.5777... ppl: 34.2019  Val Loss: 3.5323\n",
      "Epoch: 6/10... Step: 13408... Loss: 2.1590... ppl: 34.3118  Val Loss: 3.5355\n",
      "Epoch: 6/10... Step: 13440... Loss: 3.3249... ppl: 34.2607  Val Loss: 3.5340\n",
      "Epoch: 6/10... Step: 13472... Loss: 3.1664... ppl: 34.2354  Val Loss: 3.5333\n",
      "Epoch: 6/10... Step: 13504... Loss: 2.8813... ppl: 34.4193  Val Loss: 3.5386\n",
      "Epoch: 6/10... Step: 13536... Loss: 3.1341... ppl: 34.4186  Val Loss: 3.5386\n",
      "Epoch: 6/10... Step: 13568... Loss: 2.7853... ppl: 34.3916  Val Loss: 3.5378\n",
      "Epoch: 6/10... Step: 13600... Loss: 2.7768... ppl: 34.3957  Val Loss: 3.5379\n",
      "Epoch: 6/10... Step: 13632... Loss: 3.7088... ppl: 34.6862  Val Loss: 3.5463\n",
      "Epoch: 6/10... Step: 13664... Loss: 2.7758... ppl: 34.3111  Val Loss: 3.5355\n",
      "Epoch: 6/10... Step: 13696... Loss: 3.1484... ppl: 34.4208  Val Loss: 3.5387\n",
      "Epoch: 6/10... Step: 13728... Loss: 2.3883... ppl: 34.6066  Val Loss: 3.5440\n",
      "Epoch: 6/10... Step: 13760... Loss: 3.2798... ppl: 34.2483  Val Loss: 3.5336\n",
      "Epoch: 6/10... Step: 13792... Loss: 3.0739... ppl: 34.3483  Val Loss: 3.5366\n",
      "Epoch: 6/10... Step: 13824... Loss: 2.4493... ppl: 34.4418  Val Loss: 3.5393\n",
      "Epoch: 6/10... Step: 13856... Loss: 3.1858... ppl: 34.4377  Val Loss: 3.5392\n",
      "Epoch: 6/10... Step: 13888... Loss: 2.7956... ppl: 34.2693  Val Loss: 3.5342\n",
      "Epoch: 6/10... Step: 13920... Loss: 3.3432... ppl: 34.3247  Val Loss: 3.5359\n",
      "Epoch: 6/10... Step: 13952... Loss: 2.1271... ppl: 34.6407  Val Loss: 3.5450\n",
      "Epoch: 6/10... Step: 13984... Loss: 3.1149... ppl: 34.6558  Val Loss: 3.5455\n",
      "Epoch: 6/10... Step: 14016... Loss: 2.5303... ppl: 34.2798  Val Loss: 3.5346\n",
      "Epoch: 6/10... Step: 14048... Loss: 2.5100... ppl: 34.2116  Val Loss: 3.5326\n",
      "Epoch: 7/10... Step: 14080... Loss: 3.0000... ppl: 34.1637  Val Loss: 3.5312\n",
      "Epoch: 7/10... Step: 14112... Loss: 2.1198... ppl: 34.4697  Val Loss: 3.5401\n",
      "Epoch: 7/10... Step: 14144... Loss: 2.7513... ppl: 34.1837  Val Loss: 3.5317\n",
      "Epoch: 7/10... Step: 14176... Loss: 2.6689... ppl: 34.4473  Val Loss: 3.5394\n",
      "Epoch: 7/10... Step: 14208... Loss: 2.4340... ppl: 34.3663  Val Loss: 3.5371\n",
      "Epoch: 7/10... Step: 14240... Loss: 2.8095... ppl: 34.4500  Val Loss: 3.5395\n",
      "Epoch: 7/10... Step: 14272... Loss: 2.3544... ppl: 34.5411  Val Loss: 3.5422\n",
      "Epoch: 7/10... Step: 14304... Loss: 2.3735... ppl: 34.6643  Val Loss: 3.5457\n",
      "Epoch: 7/10... Step: 14336... Loss: 2.6593... ppl: 34.6356  Val Loss: 3.5449\n",
      "Epoch: 7/10... Step: 14368... Loss: 3.4922... ppl: 35.1317  Val Loss: 3.5591\n",
      "Epoch: 7/10... Step: 14400... Loss: 2.6247... ppl: 34.7257  Val Loss: 3.5475\n",
      "Epoch: 7/10... Step: 14432... Loss: 2.9436... ppl: 34.4948  Val Loss: 3.5408\n",
      "Epoch: 7/10... Step: 14464... Loss: 3.0419... ppl: 34.4073  Val Loss: 3.5383\n",
      "Epoch: 7/10... Step: 14496... Loss: 3.1492... ppl: 34.7567  Val Loss: 3.5484\n",
      "Epoch: 7/10... Step: 14528... Loss: 2.0952... ppl: 35.0727  Val Loss: 3.5574\n",
      "Epoch: 7/10... Step: 14560... Loss: 2.4698... ppl: 35.2041  Val Loss: 3.5612\n",
      "Epoch: 7/10... Step: 14592... Loss: 2.4384... ppl: 35.0978  Val Loss: 3.5581\n",
      "Epoch: 7/10... Step: 14624... Loss: 2.1891... ppl: 35.1038  Val Loss: 3.5583\n",
      "Epoch: 7/10... Step: 14656... Loss: 3.0876... ppl: 35.3595  Val Loss: 3.5656\n",
      "Epoch: 7/10... Step: 14688... Loss: 3.0239... ppl: 35.2750  Val Loss: 3.5632\n",
      "Epoch: 7/10... Step: 14720... Loss: 3.6748... ppl: 35.3735  Val Loss: 3.5660\n",
      "Epoch: 7/10... Step: 14752... Loss: 2.9460... ppl: 35.3181  Val Loss: 3.5644\n",
      "Epoch: 7/10... Step: 14784... Loss: 2.2062... ppl: 35.2575  Val Loss: 3.5627\n",
      "Epoch: 7/10... Step: 14816... Loss: 2.6685... ppl: 35.2230  Val Loss: 3.5617\n",
      "Epoch: 7/10... Step: 14848... Loss: 2.5698... ppl: 34.9528  Val Loss: 3.5540\n",
      "Epoch: 7/10... Step: 14880... Loss: 2.6776... ppl: 35.2666  Val Loss: 3.5629\n",
      "Epoch: 7/10... Step: 14912... Loss: 2.5629... ppl: 35.4758  Val Loss: 3.5689\n",
      "Epoch: 7/10... Step: 14944... Loss: 3.0305... ppl: 35.2204  Val Loss: 3.5616\n",
      "Epoch: 7/10... Step: 14976... Loss: 2.8075... ppl: 34.9177  Val Loss: 3.5530\n",
      "Epoch: 7/10... Step: 15008... Loss: 2.7744... ppl: 34.9832  Val Loss: 3.5549\n",
      "Epoch: 7/10... Step: 15040... Loss: 2.1841... ppl: 35.0046  Val Loss: 3.5555\n",
      "Epoch: 7/10... Step: 15072... Loss: 3.1425... ppl: 34.7956  Val Loss: 3.5495\n",
      "Epoch: 7/10... Step: 15104... Loss: 2.7771... ppl: 35.2850  Val Loss: 3.5635\n",
      "Epoch: 7/10... Step: 15136... Loss: 3.1743... ppl: 35.3134  Val Loss: 3.5643\n",
      "Epoch: 7/10... Step: 15168... Loss: 2.3292... ppl: 35.1574  Val Loss: 3.5598\n",
      "Epoch: 7/10... Step: 15200... Loss: 2.7003... ppl: 35.1805  Val Loss: 3.5605\n",
      "Epoch: 7/10... Step: 15232... Loss: 3.0985... ppl: 34.9017  Val Loss: 3.5525\n",
      "Epoch: 7/10... Step: 15264... Loss: 2.7583... ppl: 34.9243  Val Loss: 3.5532\n",
      "Epoch: 7/10... Step: 15296... Loss: 2.9499... ppl: 34.9911  Val Loss: 3.5551\n",
      "Epoch: 7/10... Step: 15328... Loss: 2.5823... ppl: 34.9529  Val Loss: 3.5540\n",
      "Epoch: 7/10... Step: 15360... Loss: 2.7108... ppl: 35.0636  Val Loss: 3.5572\n",
      "Epoch: 7/10... Step: 15392... Loss: 2.9160... ppl: 35.0063  Val Loss: 3.5555\n",
      "Epoch: 7/10... Step: 15424... Loss: 2.3236... ppl: 35.3693  Val Loss: 3.5658\n",
      "Epoch: 7/10... Step: 15456... Loss: 3.0342... ppl: 35.1342  Val Loss: 3.5592\n",
      "Epoch: 7/10... Step: 15488... Loss: 2.7170... ppl: 35.2308  Val Loss: 3.5619\n",
      "Epoch: 7/10... Step: 15520... Loss: 2.8984... ppl: 35.2626  Val Loss: 3.5628\n",
      "Epoch: 7/10... Step: 15552... Loss: 2.5069... ppl: 35.0394  Val Loss: 3.5565\n",
      "Epoch: 7/10... Step: 15584... Loss: 3.2391... ppl: 35.0848  Val Loss: 3.5578\n",
      "Epoch: 7/10... Step: 15616... Loss: 2.2102... ppl: 35.1813  Val Loss: 3.5605\n",
      "Epoch: 7/10... Step: 15648... Loss: 2.8245... ppl: 35.1831  Val Loss: 3.5606\n",
      "Epoch: 7/10... Step: 15680... Loss: 2.7081... ppl: 35.1101  Val Loss: 3.5585\n",
      "Epoch: 7/10... Step: 15712... Loss: 2.1474... ppl: 34.9540  Val Loss: 3.5540\n",
      "Epoch: 7/10... Step: 15744... Loss: 2.7515... ppl: 35.2181  Val Loss: 3.5616\n",
      "Epoch: 7/10... Step: 15776... Loss: 3.0282... ppl: 35.0993  Val Loss: 3.5582\n",
      "Epoch: 7/10... Step: 15808... Loss: 3.0317... ppl: 35.1373  Val Loss: 3.5593\n",
      "Epoch: 7/10... Step: 15840... Loss: 3.5418... ppl: 35.3365  Val Loss: 3.5649\n",
      "Epoch: 7/10... Step: 15872... Loss: 3.3420... ppl: 35.2041  Val Loss: 3.5612\n",
      "Epoch: 7/10... Step: 15904... Loss: 2.4544... ppl: 35.2680  Val Loss: 3.5630\n",
      "Epoch: 7/10... Step: 15936... Loss: 2.6116... ppl: 35.3492  Val Loss: 3.5653\n",
      "Epoch: 7/10... Step: 15968... Loss: 1.9490... ppl: 35.5210  Val Loss: 3.5701\n",
      "Epoch: 7/10... Step: 16000... Loss: 2.8361... ppl: 35.1388  Val Loss: 3.5593\n",
      "Epoch: 7/10... Step: 16032... Loss: 2.6554... ppl: 35.1846  Val Loss: 3.5606\n",
      "Epoch: 7/10... Step: 16064... Loss: 2.2833... ppl: 35.5192  Val Loss: 3.5701\n",
      "Epoch: 7/10... Step: 16096... Loss: 2.1753... ppl: 35.1960  Val Loss: 3.5609\n",
      "Epoch: 7/10... Step: 16128... Loss: 2.4403... ppl: 35.1566  Val Loss: 3.5598\n",
      "Epoch: 7/10... Step: 16160... Loss: 2.8127... ppl: 35.3395  Val Loss: 3.5650\n",
      "Epoch: 7/10... Step: 16192... Loss: 2.5451... ppl: 35.3940  Val Loss: 3.5665\n",
      "Epoch: 7/10... Step: 16224... Loss: 3.1081... ppl: 35.0921  Val Loss: 3.5580\n",
      "Epoch: 7/10... Step: 16256... Loss: 2.4730... ppl: 35.1403  Val Loss: 3.5593\n",
      "Epoch: 7/10... Step: 16288... Loss: 2.7478... ppl: 35.4311  Val Loss: 3.5676\n",
      "Epoch: 7/10... Step: 16320... Loss: 2.2834... ppl: 35.8331  Val Loss: 3.5789\n",
      "Epoch: 7/10... Step: 16352... Loss: 2.4494... ppl: 35.4029  Val Loss: 3.5668\n",
      "Epoch: 7/10... Step: 16384... Loss: 3.3542... ppl: 35.1743  Val Loss: 3.5603\n",
      "Epoch: 8/10... Step: 16416... Loss: 3.2818... ppl: 35.1082  Val Loss: 3.5584\n",
      "Epoch: 8/10... Step: 16448... Loss: 2.6610... ppl: 35.5707  Val Loss: 3.5715\n",
      "Epoch: 8/10... Step: 16480... Loss: 2.7214... ppl: 35.1451  Val Loss: 3.5595\n",
      "Epoch: 8/10... Step: 16512... Loss: 1.7460... ppl: 35.3079  Val Loss: 3.5641\n",
      "Epoch: 8/10... Step: 16544... Loss: 2.6313... ppl: 35.3719  Val Loss: 3.5659\n",
      "Epoch: 8/10... Step: 16576... Loss: 2.8349... ppl: 35.3405  Val Loss: 3.5650\n",
      "Epoch: 8/10... Step: 16608... Loss: 2.8798... ppl: 35.4863  Val Loss: 3.5691\n",
      "Epoch: 8/10... Step: 16640... Loss: 2.3621... ppl: 35.5275  Val Loss: 3.5703\n",
      "Epoch: 8/10... Step: 16672... Loss: 2.5894... ppl: 35.5025  Val Loss: 3.5696\n",
      "Epoch: 8/10... Step: 16704... Loss: 2.7817... ppl: 36.0831  Val Loss: 3.5858\n",
      "Epoch: 8/10... Step: 16736... Loss: 2.8758... ppl: 35.9493  Val Loss: 3.5821\n",
      "Epoch: 8/10... Step: 16768... Loss: 2.8940... ppl: 35.5346  Val Loss: 3.5705\n",
      "Epoch: 8/10... Step: 16800... Loss: 2.2427... ppl: 35.3607  Val Loss: 3.5656\n",
      "Epoch: 8/10... Step: 16832... Loss: 2.5001... ppl: 35.6014  Val Loss: 3.5724\n",
      "Epoch: 8/10... Step: 16864... Loss: 2.6041... ppl: 35.9584  Val Loss: 3.5824\n",
      "Epoch: 8/10... Step: 16896... Loss: 2.2490... ppl: 36.1704  Val Loss: 3.5882\n",
      "Epoch: 8/10... Step: 16928... Loss: 2.4678... ppl: 35.9455  Val Loss: 3.5820\n",
      "Epoch: 8/10... Step: 16960... Loss: 2.4770... ppl: 36.0783  Val Loss: 3.5857\n",
      "Epoch: 8/10... Step: 16992... Loss: 2.6297... ppl: 36.3458  Val Loss: 3.5931\n",
      "Epoch: 8/10... Step: 17024... Loss: 2.2062... ppl: 36.1405  Val Loss: 3.5874\n",
      "Epoch: 8/10... Step: 17056... Loss: 2.5733... ppl: 36.3075  Val Loss: 3.5920\n",
      "Epoch: 8/10... Step: 17088... Loss: 3.2016... ppl: 36.3042  Val Loss: 3.5919\n",
      "Epoch: 8/10... Step: 17120... Loss: 2.6703... ppl: 36.3291  Val Loss: 3.5926\n",
      "Epoch: 8/10... Step: 17152... Loss: 3.1758... ppl: 36.2451  Val Loss: 3.5903\n",
      "Epoch: 8/10... Step: 17184... Loss: 2.5019... ppl: 35.8838  Val Loss: 3.5803\n",
      "Epoch: 8/10... Step: 17216... Loss: 2.4134... ppl: 36.1910  Val Loss: 3.5888\n",
      "Epoch: 8/10... Step: 17248... Loss: 2.3219... ppl: 36.3817  Val Loss: 3.5941\n",
      "Epoch: 8/10... Step: 17280... Loss: 2.3332... ppl: 36.2079  Val Loss: 3.5893\n",
      "Epoch: 8/10... Step: 17312... Loss: 3.3988... ppl: 35.9532  Val Loss: 3.5822\n",
      "Epoch: 8/10... Step: 17344... Loss: 2.4912... ppl: 35.9799  Val Loss: 3.5830\n",
      "Epoch: 8/10... Step: 17376... Loss: 2.7277... ppl: 36.0002  Val Loss: 3.5835\n",
      "Epoch: 8/10... Step: 17408... Loss: 2.6078... ppl: 35.8474  Val Loss: 3.5793\n",
      "Epoch: 8/10... Step: 17440... Loss: 2.7917... ppl: 36.1227  Val Loss: 3.5869\n",
      "Epoch: 8/10... Step: 17472... Loss: 2.7359... ppl: 36.4116  Val Loss: 3.5949\n",
      "Epoch: 8/10... Step: 17504... Loss: 2.5854... ppl: 36.1528  Val Loss: 3.5878\n",
      "Epoch: 8/10... Step: 17536... Loss: 2.7774... ppl: 36.0484  Val Loss: 3.5849\n",
      "Epoch: 8/10... Step: 17568... Loss: 2.8222... ppl: 35.9575  Val Loss: 3.5823\n",
      "Epoch: 8/10... Step: 17600... Loss: 2.5915... ppl: 35.8052  Val Loss: 3.5781\n",
      "Epoch: 8/10... Step: 17632... Loss: 2.6206... ppl: 35.8181  Val Loss: 3.5785\n",
      "Epoch: 8/10... Step: 17664... Loss: 2.6114... ppl: 35.8135  Val Loss: 3.5783\n",
      "Epoch: 8/10... Step: 17696... Loss: 3.1739... ppl: 35.9211  Val Loss: 3.5813\n",
      "Epoch: 8/10... Step: 17728... Loss: 3.0985... ppl: 35.7971  Val Loss: 3.5779\n",
      "Epoch: 8/10... Step: 17760... Loss: 2.5652... ppl: 36.2182  Val Loss: 3.5896\n",
      "Epoch: 8/10... Step: 17792... Loss: 2.5359... ppl: 36.0487  Val Loss: 3.5849\n",
      "Epoch: 8/10... Step: 17824... Loss: 2.7530... ppl: 36.0988  Val Loss: 3.5863\n",
      "Epoch: 8/10... Step: 17856... Loss: 2.3449... ppl: 36.2447  Val Loss: 3.5903\n",
      "Epoch: 8/10... Step: 17888... Loss: 2.7778... ppl: 35.9661  Val Loss: 3.5826\n",
      "Epoch: 8/10... Step: 17920... Loss: 2.1367... ppl: 36.0074  Val Loss: 3.5837\n",
      "Epoch: 8/10... Step: 17952... Loss: 2.5677... ppl: 36.1536  Val Loss: 3.5878\n",
      "Epoch: 8/10... Step: 17984... Loss: 2.7245... ppl: 36.2524  Val Loss: 3.5905\n",
      "Epoch: 8/10... Step: 18016... Loss: 2.6451... ppl: 36.1453  Val Loss: 3.5875\n",
      "Epoch: 8/10... Step: 18048... Loss: 3.0095... ppl: 35.9767  Val Loss: 3.5829\n",
      "Epoch: 8/10... Step: 18080... Loss: 2.9379... ppl: 36.0592  Val Loss: 3.5852\n",
      "Epoch: 8/10... Step: 18112... Loss: 2.3413... ppl: 36.1445  Val Loss: 3.5875\n",
      "Epoch: 8/10... Step: 18144... Loss: 3.4175... ppl: 36.0681  Val Loss: 3.5854\n",
      "Epoch: 8/10... Step: 18176... Loss: 2.8642... ppl: 36.1569  Val Loss: 3.5879\n",
      "Epoch: 8/10... Step: 18208... Loss: 2.7750... ppl: 36.1506  Val Loss: 3.5877\n",
      "Epoch: 8/10... Step: 18240... Loss: 2.8276... ppl: 36.3136  Val Loss: 3.5922\n",
      "Epoch: 8/10... Step: 18272... Loss: 2.0540... ppl: 36.2736  Val Loss: 3.5911\n",
      "Epoch: 8/10... Step: 18304... Loss: 2.9060... ppl: 36.4215  Val Loss: 3.5952\n",
      "Epoch: 8/10... Step: 18336... Loss: 2.9847... ppl: 36.2166  Val Loss: 3.5895\n",
      "Epoch: 8/10... Step: 18368... Loss: 2.9345... ppl: 36.1426  Val Loss: 3.5875\n",
      "Epoch: 8/10... Step: 18400... Loss: 2.7282... ppl: 36.5893  Val Loss: 3.5998\n",
      "Epoch: 8/10... Step: 18432... Loss: 3.1804... ppl: 36.3059  Val Loss: 3.5920\n",
      "Epoch: 8/10... Step: 18464... Loss: 2.4394... ppl: 36.1499  Val Loss: 3.5877\n",
      "Epoch: 8/10... Step: 18496... Loss: 2.1235... ppl: 36.3641  Val Loss: 3.5936\n",
      "Epoch: 8/10... Step: 18528... Loss: 2.8560... ppl: 36.3873  Val Loss: 3.5942\n",
      "Epoch: 8/10... Step: 18560... Loss: 2.4839... ppl: 36.1715  Val Loss: 3.5883\n",
      "Epoch: 8/10... Step: 18592... Loss: 2.7087... ppl: 36.1906  Val Loss: 3.5888\n",
      "Epoch: 8/10... Step: 18624... Loss: 2.6981... ppl: 36.2423  Val Loss: 3.5902\n",
      "Epoch: 8/10... Step: 18656... Loss: 3.6395... ppl: 36.9813  Val Loss: 3.6104\n",
      "Epoch: 8/10... Step: 18688... Loss: 2.1675... ppl: 36.5901  Val Loss: 3.5998\n",
      "Epoch: 8/10... Step: 18720... Loss: 2.4992... ppl: 36.1550  Val Loss: 3.5878\n",
      "Epoch: 9/10... Step: 18752... Loss: 2.3740... ppl: 36.0415  Val Loss: 3.5847\n",
      "Epoch: 9/10... Step: 18784... Loss: 2.5728... ppl: 36.4510  Val Loss: 3.5960\n",
      "Epoch: 9/10... Step: 18816... Loss: 2.8486... ppl: 36.1464  Val Loss: 3.5876\n",
      "Epoch: 9/10... Step: 18848... Loss: 2.8535... ppl: 36.1068  Val Loss: 3.5865\n",
      "Epoch: 9/10... Step: 18880... Loss: 2.8311... ppl: 36.4074  Val Loss: 3.5948\n",
      "Epoch: 9/10... Step: 18912... Loss: 2.8633... ppl: 36.3114  Val Loss: 3.5921\n",
      "Epoch: 9/10... Step: 18944... Loss: 2.7276... ppl: 36.3946  Val Loss: 3.5944\n",
      "Epoch: 9/10... Step: 18976... Loss: 2.8569... ppl: 36.5382  Val Loss: 3.5984\n",
      "Epoch: 9/10... Step: 19008... Loss: 2.9150... ppl: 36.5618  Val Loss: 3.5990\n",
      "Epoch: 9/10... Step: 19040... Loss: 2.4347... ppl: 36.8728  Val Loss: 3.6075\n",
      "Epoch: 9/10... Step: 19072... Loss: 2.7073... ppl: 37.0713  Val Loss: 3.6128\n",
      "Epoch: 9/10... Step: 19104... Loss: 2.5709... ppl: 36.4533  Val Loss: 3.5960\n",
      "Epoch: 9/10... Step: 19136... Loss: 1.9160... ppl: 36.2510  Val Loss: 3.5905\n",
      "Epoch: 9/10... Step: 19168... Loss: 2.3123... ppl: 36.4422  Val Loss: 3.5957\n",
      "Epoch: 9/10... Step: 19200... Loss: 2.9977... ppl: 36.8820  Val Loss: 3.6077\n",
      "Epoch: 9/10... Step: 19232... Loss: 2.9386... ppl: 37.2697  Val Loss: 3.6182\n",
      "Epoch: 9/10... Step: 19264... Loss: 2.9088... ppl: 37.0604  Val Loss: 3.6126\n",
      "Epoch: 9/10... Step: 19296... Loss: 3.2458... ppl: 37.1060  Val Loss: 3.6138\n",
      "Epoch: 9/10... Step: 19328... Loss: 2.8607... ppl: 37.2330  Val Loss: 3.6172\n",
      "Epoch: 9/10... Step: 19360... Loss: 2.9011... ppl: 37.2979  Val Loss: 3.6189\n",
      "Epoch: 9/10... Step: 19392... Loss: 2.2870... ppl: 37.4088  Val Loss: 3.6219\n",
      "Epoch: 9/10... Step: 19424... Loss: 2.8298... ppl: 37.4175  Val Loss: 3.6221\n",
      "Epoch: 9/10... Step: 19456... Loss: 2.6120... ppl: 37.5113  Val Loss: 3.6246\n",
      "Epoch: 9/10... Step: 19488... Loss: 2.4044... ppl: 37.5234  Val Loss: 3.6250\n",
      "Epoch: 9/10... Step: 19520... Loss: 2.6633... ppl: 37.1035  Val Loss: 3.6137\n",
      "Epoch: 9/10... Step: 19552... Loss: 2.2498... ppl: 37.2234  Val Loss: 3.6169\n",
      "Epoch: 9/10... Step: 19584... Loss: 2.2387... ppl: 37.4572  Val Loss: 3.6232\n",
      "Epoch: 9/10... Step: 19616... Loss: 2.8884... ppl: 37.3450  Val Loss: 3.6202\n",
      "Epoch: 9/10... Step: 19648... Loss: 2.9791... ppl: 37.0694  Val Loss: 3.6128\n",
      "Epoch: 9/10... Step: 19680... Loss: 2.7392... ppl: 36.9751  Val Loss: 3.6102\n",
      "Epoch: 9/10... Step: 19712... Loss: 2.2265... ppl: 37.1096  Val Loss: 3.6139\n",
      "Epoch: 9/10... Step: 19744... Loss: 2.0589... ppl: 36.8865  Val Loss: 3.6078\n",
      "Epoch: 9/10... Step: 19776... Loss: 3.0137... ppl: 36.9269  Val Loss: 3.6089\n",
      "Epoch: 9/10... Step: 19808... Loss: 2.0728... ppl: 37.3612  Val Loss: 3.6206\n",
      "Epoch: 9/10... Step: 19840... Loss: 2.3794... ppl: 37.2135  Val Loss: 3.6167\n",
      "Epoch: 9/10... Step: 19872... Loss: 2.7288... ppl: 37.1532  Val Loss: 3.6150\n",
      "Epoch: 9/10... Step: 19904... Loss: 2.4424... ppl: 37.0602  Val Loss: 3.6125\n",
      "Epoch: 9/10... Step: 19936... Loss: 2.4504... ppl: 36.8215  Val Loss: 3.6061\n",
      "Epoch: 9/10... Step: 19968... Loss: 2.4873... ppl: 36.8193  Val Loss: 3.6060\n",
      "Epoch: 9/10... Step: 20000... Loss: 3.0474... ppl: 36.9685  Val Loss: 3.6101\n",
      "Epoch: 9/10... Step: 20032... Loss: 2.8525... ppl: 36.8805  Val Loss: 3.6077\n",
      "Epoch: 9/10... Step: 20064... Loss: 2.5170... ppl: 36.8090  Val Loss: 3.6057\n",
      "Epoch: 9/10... Step: 20096... Loss: 2.6391... ppl: 37.0496  Val Loss: 3.6123\n",
      "Epoch: 9/10... Step: 20128... Loss: 2.7922... ppl: 37.2933  Val Loss: 3.6188\n",
      "Epoch: 9/10... Step: 20160... Loss: 2.8190... ppl: 37.1592  Val Loss: 3.6152\n",
      "Epoch: 9/10... Step: 20192... Loss: 2.2424... ppl: 37.3192  Val Loss: 3.6195\n",
      "Epoch: 9/10... Step: 20224... Loss: 2.4621... ppl: 37.1481  Val Loss: 3.6149\n",
      "Epoch: 9/10... Step: 20256... Loss: 2.3181... ppl: 36.9296  Val Loss: 3.6090\n",
      "Epoch: 9/10... Step: 20288... Loss: 2.7638... ppl: 37.1372  Val Loss: 3.6146\n",
      "Epoch: 9/10... Step: 20320... Loss: 2.6802... ppl: 37.3911  Val Loss: 3.6214\n",
      "Epoch: 9/10... Step: 20352... Loss: 2.4383... ppl: 37.2118  Val Loss: 3.6166\n",
      "Epoch: 9/10... Step: 20384... Loss: 2.8461... ppl: 37.0409  Val Loss: 3.6120\n",
      "Epoch: 9/10... Step: 20416... Loss: 2.6297... ppl: 37.1277  Val Loss: 3.6144\n",
      "Epoch: 9/10... Step: 20448... Loss: 2.5675... ppl: 37.2779  Val Loss: 3.6184\n",
      "Epoch: 9/10... Step: 20480... Loss: 2.5221... ppl: 37.1928  Val Loss: 3.6161\n",
      "Epoch: 9/10... Step: 20512... Loss: 2.7059... ppl: 37.1820  Val Loss: 3.6158\n",
      "Epoch: 9/10... Step: 20544... Loss: 2.5329... ppl: 37.2777  Val Loss: 3.6184\n",
      "Epoch: 9/10... Step: 20576... Loss: 2.4740... ppl: 37.3968  Val Loss: 3.6216\n",
      "Epoch: 9/10... Step: 20608... Loss: 2.4769... ppl: 37.3281  Val Loss: 3.6197\n",
      "Epoch: 9/10... Step: 20640... Loss: 2.3322... ppl: 37.4758  Val Loss: 3.6237\n",
      "Epoch: 9/10... Step: 20672... Loss: 2.8209... ppl: 37.4418  Val Loss: 3.6228\n",
      "Epoch: 9/10... Step: 20704... Loss: 2.8033... ppl: 37.1324  Val Loss: 3.6145\n",
      "Epoch: 9/10... Step: 20736... Loss: 2.5004... ppl: 37.4558  Val Loss: 3.6232\n",
      "Epoch: 9/10... Step: 20768... Loss: 2.9370... ppl: 37.4979  Val Loss: 3.6243\n",
      "Epoch: 9/10... Step: 20800... Loss: 2.8074... ppl: 37.1569  Val Loss: 3.6151\n",
      "Epoch: 9/10... Step: 20832... Loss: 2.8982... ppl: 37.3411  Val Loss: 3.6201\n",
      "Epoch: 9/10... Step: 20864... Loss: 2.4445... ppl: 37.4126  Val Loss: 3.6220\n",
      "Epoch: 9/10... Step: 20896... Loss: 2.6448... ppl: 37.2983  Val Loss: 3.6189\n",
      "Epoch: 9/10... Step: 20928... Loss: 2.5297... ppl: 37.2890  Val Loss: 3.6187\n",
      "Epoch: 9/10... Step: 20960... Loss: 2.5248... ppl: 37.1811  Val Loss: 3.6158\n",
      "Epoch: 9/10... Step: 20992... Loss: 2.3322... ppl: 37.8946  Val Loss: 3.6348\n",
      "Epoch: 9/10... Step: 21024... Loss: 2.3758... ppl: 37.8711  Val Loss: 3.6342\n",
      "Epoch: 9/10... Step: 21056... Loss: 2.5750... ppl: 37.3404  Val Loss: 3.6201\n",
      "Epoch: 10/10... Step: 21088... Loss: 2.7246... ppl: 37.2013  Val Loss: 3.6163\n",
      "Epoch: 10/10... Step: 21120... Loss: 2.4612... ppl: 37.4479  Val Loss: 3.6230\n",
      "Epoch: 10/10... Step: 21152... Loss: 2.6608... ppl: 37.2868  Val Loss: 3.6186\n",
      "Epoch: 10/10... Step: 21184... Loss: 2.6358... ppl: 37.1741  Val Loss: 3.6156\n",
      "Epoch: 10/10... Step: 21216... Loss: 2.2345... ppl: 37.3689  Val Loss: 3.6208\n",
      "Epoch: 10/10... Step: 21248... Loss: 2.4369... ppl: 37.4062  Val Loss: 3.6218\n",
      "Epoch: 10/10... Step: 21280... Loss: 3.1786... ppl: 37.5058  Val Loss: 3.6245\n",
      "Epoch: 10/10... Step: 21312... Loss: 2.6270... ppl: 37.5067  Val Loss: 3.6245\n",
      "Epoch: 10/10... Step: 21344... Loss: 2.6310... ppl: 37.7216  Val Loss: 3.6302\n",
      "Epoch: 10/10... Step: 21376... Loss: 2.4427... ppl: 37.7082  Val Loss: 3.6299\n",
      "Epoch: 10/10... Step: 21408... Loss: 2.5592... ppl: 38.2792  Val Loss: 3.6449\n",
      "Epoch: 10/10... Step: 21440... Loss: 2.7077... ppl: 37.7577  Val Loss: 3.6312\n",
      "Epoch: 10/10... Step: 21472... Loss: 2.5925... ppl: 37.4054  Val Loss: 3.6218\n",
      "Epoch: 10/10... Step: 21504... Loss: 2.3823... ppl: 37.4070  Val Loss: 3.6219\n",
      "Epoch: 10/10... Step: 21536... Loss: 2.3432... ppl: 37.9357  Val Loss: 3.6359\n",
      "Epoch: 10/10... Step: 21568... Loss: 2.6896... ppl: 38.3604  Val Loss: 3.6470\n",
      "Epoch: 10/10... Step: 21600... Loss: 2.3593... ppl: 38.2312  Val Loss: 3.6437\n",
      "Epoch: 10/10... Step: 21632... Loss: 2.4544... ppl: 38.3615  Val Loss: 3.6471\n",
      "Epoch: 10/10... Step: 21664... Loss: 2.4274... ppl: 38.3655  Val Loss: 3.6472\n",
      "Epoch: 10/10... Step: 21696... Loss: 3.0429... ppl: 38.5864  Val Loss: 3.6529\n",
      "Epoch: 10/10... Step: 21728... Loss: 2.4079... ppl: 38.5969  Val Loss: 3.6532\n",
      "Epoch: 10/10... Step: 21760... Loss: 2.2194... ppl: 38.5054  Val Loss: 3.6508\n",
      "Epoch: 10/10... Step: 21792... Loss: 2.3112... ppl: 38.6216  Val Loss: 3.6538\n",
      "Epoch: 10/10... Step: 21824... Loss: 2.5628... ppl: 38.7515  Val Loss: 3.6572\n",
      "Epoch: 10/10... Step: 21856... Loss: 2.4568... ppl: 38.3942  Val Loss: 3.6479\n",
      "Epoch: 10/10... Step: 21888... Loss: 2.3212... ppl: 38.2981  Val Loss: 3.6454\n",
      "Epoch: 10/10... Step: 21920... Loss: 2.8682... ppl: 38.4372  Val Loss: 3.6490\n",
      "Epoch: 10/10... Step: 21952... Loss: 2.8257... ppl: 38.6574  Val Loss: 3.6547\n",
      "Epoch: 10/10... Step: 21984... Loss: 1.9787... ppl: 38.2953  Val Loss: 3.6453\n",
      "Epoch: 10/10... Step: 22016... Loss: 2.6070... ppl: 38.1047  Val Loss: 3.6403\n",
      "Epoch: 10/10... Step: 22048... Loss: 2.2203... ppl: 38.2324  Val Loss: 3.6437\n",
      "Epoch: 10/10... Step: 22080... Loss: 2.7807... ppl: 38.0524  Val Loss: 3.6390\n",
      "Epoch: 10/10... Step: 22112... Loss: 2.0832... ppl: 37.8524  Val Loss: 3.6337\n",
      "Epoch: 10/10... Step: 22144... Loss: 2.3859... ppl: 38.4486  Val Loss: 3.6493\n",
      "Epoch: 10/10... Step: 22176... Loss: 2.2127... ppl: 38.2874  Val Loss: 3.6451\n",
      "Epoch: 10/10... Step: 22208... Loss: 2.8405... ppl: 38.1823  Val Loss: 3.6424\n",
      "Epoch: 10/10... Step: 22240... Loss: 2.5521... ppl: 38.3699  Val Loss: 3.6473\n",
      "Epoch: 10/10... Step: 22272... Loss: 2.6456... ppl: 37.8834  Val Loss: 3.6345\n",
      "Epoch: 10/10... Step: 22304... Loss: 2.6610... ppl: 37.9278  Val Loss: 3.6357\n",
      "Epoch: 10/10... Step: 22336... Loss: 2.4772... ppl: 37.9331  Val Loss: 3.6358\n",
      "Epoch: 10/10... Step: 22368... Loss: 2.9071... ppl: 37.9538  Val Loss: 3.6364\n",
      "Epoch: 10/10... Step: 22400... Loss: 2.4358... ppl: 37.9230  Val Loss: 3.6356\n",
      "Epoch: 10/10... Step: 22432... Loss: 2.3941... ppl: 38.0645  Val Loss: 3.6393\n",
      "Epoch: 10/10... Step: 22464... Loss: 1.7576... ppl: 38.4195  Val Loss: 3.6486\n",
      "Epoch: 10/10... Step: 22496... Loss: 2.4332... ppl: 38.1551  Val Loss: 3.6417\n",
      "Epoch: 10/10... Step: 22528... Loss: 2.8532... ppl: 38.3750  Val Loss: 3.6474\n",
      "Epoch: 10/10... Step: 22560... Loss: 2.5803... ppl: 38.1845  Val Loss: 3.6424\n",
      "Epoch: 10/10... Step: 22592... Loss: 2.3251... ppl: 37.9308  Val Loss: 3.6358\n",
      "Epoch: 10/10... Step: 22624... Loss: 2.6839... ppl: 38.1677  Val Loss: 3.6420\n",
      "Epoch: 10/10... Step: 22656... Loss: 2.3714... ppl: 38.3799  Val Loss: 3.6475\n",
      "Epoch: 10/10... Step: 22688... Loss: 2.7712... ppl: 38.1659  Val Loss: 3.6419\n",
      "Epoch: 10/10... Step: 22720... Loss: 2.4279... ppl: 38.2115  Val Loss: 3.6431\n",
      "Epoch: 10/10... Step: 22752... Loss: 2.8585... ppl: 38.2211  Val Loss: 3.6434\n",
      "Epoch: 10/10... Step: 22784... Loss: 2.7593... ppl: 38.3989  Val Loss: 3.6480\n",
      "Epoch: 10/10... Step: 22816... Loss: 2.6302... ppl: 38.2766  Val Loss: 3.6448\n",
      "Epoch: 10/10... Step: 22848... Loss: 2.7946... ppl: 38.2207  Val Loss: 3.6434\n",
      "Epoch: 10/10... Step: 22880... Loss: 2.0854... ppl: 38.4469  Val Loss: 3.6493\n",
      "Epoch: 10/10... Step: 22912... Loss: 1.8666... ppl: 38.4377  Val Loss: 3.6490\n",
      "Epoch: 10/10... Step: 22944... Loss: 2.8571... ppl: 38.3026  Val Loss: 3.6455\n",
      "Epoch: 10/10... Step: 22976... Loss: 2.4398... ppl: 38.4148  Val Loss: 3.6484\n",
      "Epoch: 10/10... Step: 23008... Loss: 2.5867... ppl: 38.6633  Val Loss: 3.6549\n",
      "Epoch: 10/10... Step: 23040... Loss: 2.2128... ppl: 38.1966  Val Loss: 3.6427\n",
      "Epoch: 10/10... Step: 23072... Loss: 2.6528... ppl: 38.4025  Val Loss: 3.6481\n",
      "Epoch: 10/10... Step: 23104... Loss: 2.4865... ppl: 38.7290  Val Loss: 3.6566\n",
      "Epoch: 10/10... Step: 23136... Loss: 2.4416... ppl: 38.3634  Val Loss: 3.6471\n",
      "Epoch: 10/10... Step: 23168... Loss: 2.5970... ppl: 38.3322  Val Loss: 3.6463\n",
      "Epoch: 10/10... Step: 23200... Loss: 2.2597... ppl: 38.5355  Val Loss: 3.6516\n",
      "Epoch: 10/10... Step: 23232... Loss: 2.3920... ppl: 38.4317  Val Loss: 3.6489\n",
      "Epoch: 10/10... Step: 23264... Loss: 2.4861... ppl: 38.1832  Val Loss: 3.6424\n",
      "Epoch: 10/10... Step: 23296... Loss: 2.5970... ppl: 38.2185  Val Loss: 3.6433\n",
      "Epoch: 10/10... Step: 23328... Loss: 2.4317... ppl: 38.9074  Val Loss: 3.6612\n",
      "Epoch: 10/10... Step: 23360... Loss: 3.0785... ppl: 38.9220  Val Loss: 3.6616\n",
      "Epoch: 10/10... Step: 23392... Loss: 2.5479... ppl: 38.5554  Val Loss: 3.6521\n",
      "Epoch: 10/10... Step: 23424... Loss: 2.0032... ppl: 38.2466  Val Loss: 3.6441\n"
     ]
    }
   ],
   "source": [
    "# specify batch size\n",
    "batch_size = 64\n",
    "\n",
    "# train the model\n",
    "train(net, batch_size = batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ6leiAm0Wmn"
   },
   "source": [
    "# TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1614664062750,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "FPoIBPQpgqZy",
    "outputId": "c2bf11ad-bc42-4bf5-eb54-46c3dfd36fb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'saved_weights.pt'\n",
    "net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1614664674574,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "bch2a0WglQ96"
   },
   "outputs": [],
   "source": [
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "      inputs = inputs.cuda()\n",
    "\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  if(torch.cuda.is_available()):\n",
    "      p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
    "  \n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1614664696173,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "EQR-euTiFan9"
   },
   "outputs": [],
   "source": [
    "def sample(net, size = 2, seed_text='it is'):\n",
    "        \n",
    "    if(torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = seed_text.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in toks:\n",
    "      token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1614664775051,
     "user": {
      "displayName": "aniket poojari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggq_cw5GlJ_HceHGjFtLxCCLPqiYvg5ffKS76adUw=s64",
      "userId": "09624879631889696305"
     },
     "user_tz": -330
    },
    "id": "VSakRw3SHRv2",
    "outputId": "d49abe8c-c987-4b19-efef-7eda7850bc39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed text: i want to >> output: i want to order a coffee drink from starbucks for me please order\n"
     ]
    }
   ],
   "source": [
    "# seed texts\n",
    "seeds = [\"i want to\"]\n",
    "\n",
    "# number of tokens to generate\n",
    "num_toks = 10\n",
    "\n",
    "for s in seeds:\n",
    "  text_gen = sample(net, num_toks, seed_text = s)\n",
    "  print(\"seed text:\", s, \">> output:\",text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x61pT-Zodg3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-Generation-Using-Neural-Language-Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0619c4ac931941a5a2aff2e4a352f793": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79b97eb00f1c4b0fb24a63f8a9558f4b",
      "placeholder": "",
      "style": "IPY_MODEL_32d382d3351e4d2da606c0eb54a98660",
      "value": " 64776/64776 [09:09&lt;00:00, 117.84it/s]"
     }
    },
    "32d382d3351e4d2da606c0eb54a98660": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40f2c3a49a4c4026a0b107b8c2b6ec6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7511ca3a1b4249c18b725a02594ea5e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40f2c3a49a4c4026a0b107b8c2b6ec6c",
      "max": 64776,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a240d19768074d049cae40af9ac869e4",
      "value": 64776
     }
    },
    "79b97eb00f1c4b0fb24a63f8a9558f4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a240d19768074d049cae40af9ac869e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae48cfe6dc1b44829c03769bf7b6583b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7511ca3a1b4249c18b725a02594ea5e6",
       "IPY_MODEL_0619c4ac931941a5a2aff2e4a352f793"
      ],
      "layout": "IPY_MODEL_f940c82a1d2a4317bd18be329d9280c6"
     }
    },
    "f940c82a1d2a4317bd18be329d9280c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
